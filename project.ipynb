{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["EoC7XDIb2ODN"],"machine_shape":"hm","gpuType":"V100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"1987063f9aa74301aa986afeda773094":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6d0762a3a9a24be9b3b685b314234c4d","IPY_MODEL_eba34e134a634ed4adbd843aa29b77d0","IPY_MODEL_7472bfeb06b24fbe94c02a46e295d421"],"layout":"IPY_MODEL_8785699d2a124594a3c21b6332871e6b"}},"6d0762a3a9a24be9b3b685b314234c4d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_03a28944279b4f3494c0dad3da53ee0f","placeholder":"​","style":"IPY_MODEL_8000fcd65a5d4780a7c1c7b72827c877","value":"Downloading model.safetensors: 100%"}},"eba34e134a634ed4adbd843aa29b77d0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ebffb63d5696411cbae0cfc65461f9ca","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_abd8308d088345fcab985b8353306f02","value":440449768}},"7472bfeb06b24fbe94c02a46e295d421":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_da021e63a8724d9ca736186596ca162f","placeholder":"​","style":"IPY_MODEL_2a225b8640b74170be280bd1642d2119","value":" 440M/440M [00:01&lt;00:00, 408MB/s]"}},"8785699d2a124594a3c21b6332871e6b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03a28944279b4f3494c0dad3da53ee0f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8000fcd65a5d4780a7c1c7b72827c877":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ebffb63d5696411cbae0cfc65461f9ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"abd8308d088345fcab985b8353306f02":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"da021e63a8724d9ca736186596ca162f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a225b8640b74170be280bd1642d2119":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4c0391611c9e40368966a36612007645":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6aadfee69e184d459baa97f159e6e925","IPY_MODEL_be244f788f394dd4b39f84fc3464c2f0","IPY_MODEL_b009753ec1d647789211ff24e5f86754"],"layout":"IPY_MODEL_a96d854d3c224c6686f6a4506d6193e0"}},"6aadfee69e184d459baa97f159e6e925":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3132306d83b94684b3a723255f99740c","placeholder":"​","style":"IPY_MODEL_4179b1bb81ea406a95913a4c3bc14d08","value":"Downloading (…)okenizer_config.json: 100%"}},"be244f788f394dd4b39f84fc3464c2f0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5537e6d2df0841169b5e7b6a4d5af8b6","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4448011001bf4af89ae30ed69ba910c9","value":28}},"b009753ec1d647789211ff24e5f86754":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_76a74c4d60584b90bb4373bb03a7278b","placeholder":"​","style":"IPY_MODEL_e6f14c719af24f0ab7a217db5107c008","value":" 28.0/28.0 [00:00&lt;00:00, 2.36kB/s]"}},"a96d854d3c224c6686f6a4506d6193e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3132306d83b94684b3a723255f99740c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4179b1bb81ea406a95913a4c3bc14d08":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5537e6d2df0841169b5e7b6a4d5af8b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4448011001bf4af89ae30ed69ba910c9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"76a74c4d60584b90bb4373bb03a7278b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6f14c719af24f0ab7a217db5107c008":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9d88969feeb24d9cab1b851a27164c90":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_534daa3ab27248db8e9ef586f92bfc9a","IPY_MODEL_31af7f9fa05f4631a408386c747d27ce","IPY_MODEL_dc0dd1ac32f7431e8a2f3214fba5bcb8"],"layout":"IPY_MODEL_e0dd1c1cf0cc4717b187b43e0b1ef847"}},"534daa3ab27248db8e9ef586f92bfc9a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e52516bab7242a08eeb562eea04ca24","placeholder":"​","style":"IPY_MODEL_278532f611834698ab4546332c3a86de","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"31af7f9fa05f4631a408386c747d27ce":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_74537096b29549a59d99487c5ca667a5","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b1b57316e5924d8281aa965ff9a126d1","value":231508}},"dc0dd1ac32f7431e8a2f3214fba5bcb8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b11bdbd8c70438daebbc10bb48cac77","placeholder":"​","style":"IPY_MODEL_99695df336e441f2ab944c4f01593dc7","value":" 232k/232k [00:00&lt;00:00, 5.43MB/s]"}},"e0dd1c1cf0cc4717b187b43e0b1ef847":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e52516bab7242a08eeb562eea04ca24":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"278532f611834698ab4546332c3a86de":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"74537096b29549a59d99487c5ca667a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1b57316e5924d8281aa965ff9a126d1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4b11bdbd8c70438daebbc10bb48cac77":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99695df336e441f2ab944c4f01593dc7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"33017f1765d643678016d0738b4b528b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cde790f3d03c4abdb26e5af6818863b7","IPY_MODEL_c6573c1038234679949049c4266d42da","IPY_MODEL_4d0150ede6e44cbf94d0b3643d1495a9"],"layout":"IPY_MODEL_65fc45284b2448449831ce70341d18e9"}},"cde790f3d03c4abdb26e5af6818863b7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4e9186b5fab44cb88d91d0749b8bb3ce","placeholder":"​","style":"IPY_MODEL_13ec26994bd84bac9b6083751ada8910","value":"Downloading (…)/main/tokenizer.json: 100%"}},"c6573c1038234679949049c4266d42da":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0fb54aaaccfd4f7d94da9c4d6efdd42f","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fddccf6150eb48469b1b48094d91c46f","value":466062}},"4d0150ede6e44cbf94d0b3643d1495a9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3084fab0aa25451f9c5072c23d4a3968","placeholder":"​","style":"IPY_MODEL_4bea1eccf4db453d8151ace93ea1a2bc","value":" 466k/466k [00:00&lt;00:00, 5.95MB/s]"}},"65fc45284b2448449831ce70341d18e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e9186b5fab44cb88d91d0749b8bb3ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13ec26994bd84bac9b6083751ada8910":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0fb54aaaccfd4f7d94da9c4d6efdd42f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fddccf6150eb48469b1b48094d91c46f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3084fab0aa25451f9c5072c23d4a3968":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4bea1eccf4db453d8151ace93ea1a2bc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cb7398583a5a45e08ae481c2ed094632":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7bacf61593bf40a1a2d1cf2d530a86d5","IPY_MODEL_46a081d21fc24a7580ca2c8cb555edc4","IPY_MODEL_2a2267cca7914a699870ffafee8585c7"],"layout":"IPY_MODEL_f25eb2b895c244ba829ee939d3a6f1f9"}},"7bacf61593bf40a1a2d1cf2d530a86d5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_23dcf4fad2764906a22850466b0e4c40","placeholder":"​","style":"IPY_MODEL_358e6ae07d02446cb233260ee605dd62","value":"Downloading (…)lve/main/config.json: 100%"}},"46a081d21fc24a7580ca2c8cb555edc4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba6c17a554124c8d8a3458fcaf0f57fa","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e09e6ec9fbf542ab99fca107668e19b3","value":570}},"2a2267cca7914a699870ffafee8585c7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_35a1ddfc71834c93ac0e9cdf286b5f69","placeholder":"​","style":"IPY_MODEL_da7c61e5eda141b687b892a16cc012c1","value":" 570/570 [00:00&lt;00:00, 56.0kB/s]"}},"f25eb2b895c244ba829ee939d3a6f1f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23dcf4fad2764906a22850466b0e4c40":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"358e6ae07d02446cb233260ee605dd62":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ba6c17a554124c8d8a3458fcaf0f57fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e09e6ec9fbf542ab99fca107668e19b3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"35a1ddfc71834c93ac0e9cdf286b5f69":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da7c61e5eda141b687b892a16cc012c1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Server Execution"],"metadata":{"id":"EoC7XDIb2ODN"}},{"cell_type":"markdown","source":["## Connecting to server\n","The servers names are c-001 to c-008, you need to connect to them through GlobalProtect VPN (password from phone) or through SSH proxy at gate.tau.ac.il\n","I use `c-002.cs.tau.ac.il` because the `c-001` doesn't work: \\\\\n","`ssh <tau_username>@c-002.cs.tau.ac.il`\n","\n","### Paths on server\n","* Home directory: `/a/home/cc/students/cs/<tau_username>`, or `~`. \\\\\n","has a little storage so we don't use it\n","* Storage directory: `/vol/joberant_nobck/data/NLP_368307701_2223/`"],"metadata":{"id":"DnEKy-rjnk8a"}},{"cell_type":"markdown","source":["## Obtain code and data (RUN ONCE)\n","To download the code, and initialise a symbolic link to the data from Amit's directory (without copying it):\n","```\n","cd /vol/joberant_nobck/data/NLP_368307701_2223/<tau_username>\n","git clone https://www.github.com/pazbenitzhak/LyricsToChordsGenerator.git\n","cd LyricsToChordsGenerator\n","ln -s ../../amittal/dataset/ .\n","```"],"metadata":{"id":"qWl9CxMUqRIS"}},{"cell_type":"markdown","source":["## Setup environment\n","### Install MobaXterm\n","MobaXterm is a SSH client with FTP (insead of PuTTY + WinSCP), you can drug files in/out. \\\\\n","In addition, it supports SSH proxy, and remembers passwords - that means you can connect to the server in one click.\n","1. Download and install it from here:\n","https://mobaxterm.mobatek.net/download-home-edition.html\n","\n","2. Open the software, and create a new session using Sessions->New Session\n","\n","3. It will open a Session settings window, choose the SSH tab.\n","\n","4. Fill: \\\\\n","Remote host: c-002.cs.tau.ac.il \\\\\n","Set \"Specify username\", and write your tau username account \\\\\n","5. Under \"Network settings\", click on \"SSH gateway (jump host)\". Fill it: \\\\\n","Gateway host: gate.tau.ac.il \\\\\n","Username: <your tau username> \\\\\n","Press OK\n","\n","6. Press OK again to save your settings.\n","\n","7. Now in the left you will see the c-002.cs.tau.ac.il under \"User Sessions\". If you don't see, click the Start icon on the left and it will open your saved session.\n","\n","8. Double click c-002.cs.tau.ac.il session, and login with your TAU password (twice - for the gate, and for the server). \\\\\n","I recommend saving the password\n","\n","### Configure tmux\n","tmux (Terminal MUltiplieXer) is a linux software enables you to use multiple terminals in one screen.  \\\\\n","Default configuration has bind key Ctrl+b prefir for each command, I changed it Ctrl+a in my configuration. Default uses % for split - I changed it to s. Default uses \" for vertical split - I changed it to v. \\\\\n","To obtain my configuration execute:\n","```\n","cp /home/joberant/NLP_2223/assafgadish/.tmux.conf ~\n","```\n","Now apply it:\n","```\n","Ctrl+b\n",":source ~/.tmux.conf\n","```\n","Close the terminal and reop\n","(Note: if you already applied it once, the bind key has changed to Ctrl+a instead of Ctrl+b)\n","\n","### Create cache dir\n","In your personal directory create once a backup directory:\n","```\n","cd /home/joberant/NLP_2223/<tau_username>\n","mkdir cache_dirs\n","```\n","In EVERY terminal befor you execute the script, you must run:\n","```\n","export XDG_CACHE_HOME=/home/joberant/NLP_2223/<tau_username>/cache_dirs\n","export TORCH_HOME=/home/joberant/NLP_2223/<tau_username>/cache_dirs\n","```\n"],"metadata":{"id":"BPZytqbZ3rNG"}},{"cell_type":"markdown","source":["## Terminal Utilities\n","### tmux\n","tmux (Terminal MUltiplieXer) is a linux software enables you to use multiple terminals in one screen.  \\\\\n","Default configuration has bind key Ctrl+b prefir for each command, I changed it Ctrl+a in my configuration. Default uses % for split - I changed it to s. Default uses \" for vertical split - I changed it to v. \\\\\n","To install my configuration see above \"Configure tmux\" (should execute once). \\\\\n","To run tmux:\n","```\n","tmux -2\n","```\n","\n","Full cheatsheet: https://tmuxcheatsheet.com/ \\\\\n","Basic cheatsheet: \\\\\n","* `Ctrl+a s`: split the window horizontally\n","* `Ctrl+a v`: split the window vertically\n","* `Ctrl+a <arrow>`: switch window in the direction of the arrow (right/left/up/down)\n","\n","### Useful linux commands\n","* `watch -n1 \"<command>\"`: executes <command> every 1 second (can adjust argument) and refresh the results\n","* `tail -F <filename>`: cat the file content and output appended data as file grows\n","\n","### Setup conda\n","Must be done in every new terminal, you can add it you your ~/.bashrc if you want. \\\\\n","Initialize conda and activate our environment using Assaf's installation and environment: (notice the '.' at the beginning): \\\\\n","```\n",". /home/joberant/NLP_2223/assafgadish/anaconda3/etc/profile.d/conda.sh\n","conda init bash\n","conda activate nlp_project\n","```\n","### Create cache dir\n","In your personal directory create once a backup directory:\n","```\n","cd /home/joberant/NLP_2223/<tau_username>\n","mkdir cache_dirs\n","```\n","In EVERY terminal befor you execute the script, you must run:\n","```\n","export XDG_CACHE_HOME=/home/joberant/NLP_2223/<tau_username>/cache_dirs\n","export TORCH_HOME=/home/joberant/NLP_2223/<tau_username>/cache_dirs\n","```\n","\n","### Using Slurm\n","Full TAU tutorial: https://www.cs.tau.ac.il/system/slurm\n","* `squeue --me` - show your jobs in the queue/executing\n","* `squeue` - show everyone's jobs in the queue/executing\n","* `sbatch project.slum` - execute the project\n","* `scancel <jobid>` - kill a job. obtrain its jobid from `squeue`\n"],"metadata":{"id":"5JOaj-0StCDJ"}},{"cell_type":"markdown","source":["## Running the project\n","1. Open tmux:\n","```\n","tmux -2\n","```\n","\n","2. Split the window to 4 terminals:\n","```\n","Ctrl+a s\n","Ctrl+a s\n","Ctrl+a <down>\n","Ctrl+a s\n","Ctrl+a s\n","```\n","3. Prepare helper terminals - in all the terminals CD to your dir:\n","In all the windows run:\n","```\n","cd /vol/joberant_nobck/data/NLP_368307701_2223/<your_tau_username>/LyricsToChordsGenerator\n","```\n","Choose 3 from the 4, and in each window run a different command:\n","```\n","tail -F results/project.out\n","```\n","```\n","tail -F results/project.err\n","```\n","```\n","watch -n1 \"squeue --me\"\n","```\n","4. **Prepare execution terminal**: \\\\\n","First initialise conda:\n","```\n",". /home/joberant/NLP_2223/assafgadish/anaconda3/etc/profile.d/conda.sh\n","conda init bash\n","conda activate nlp_project\n","```\n","Now make sure you created a cache dir (see Create a cache dir), and execute once in your current termianl within tmux:\n","```\n","export XDG_CACHE_HOME=/home/joberant/NLP_2223/<tau_username>/cache_dirs\n","export TORCH_HOME=/home/joberant/NLP_2223/<tau_username>/cache_dirs\n","```\n","Now the current terminal is ready and you can execute the project many times:\n","```\n","sbatch project.slurm\n","```"],"metadata":{"id":"qFwqJWF2vLRA"}},{"cell_type":"markdown","source":["# Colab Execution"],"metadata":{"id":"YuEtSB8xp5X1"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"N16WrrxHayDE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697901902541,"user_tz":-180,"elapsed":54129,"user":{"displayName":"Amit Tal","userId":"10134168706322622051"}},"outputId":"acd679d4-8fcd-4894-e091-4c645771019a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/My Drive/nlp/project\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd '/content/drive/My Drive/nlp/project'"]},{"cell_type":"code","source":["!pip install transformers\n","!pip install \"jax<=0.3.16\" \"jaxlib<=0.3.16\"\n","!pip install torchtext==0.6\n","!pip install datasets"],"metadata":{"id":"2_u4FnkFa9Se","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b675ce07-41c3-4f0c-f625-c3fa755f91f9","executionInfo":{"status":"ok","timestamp":1697901939169,"user_tz":-180,"elapsed":36632,"user":{"displayName":"Amit Tal","userId":"10134168706322622051"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n","Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n","  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers<0.15,>=0.14 (from transformers)\n","  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m112.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n","  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n","Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.1\n","Collecting jax<=0.3.16\n","  Downloading jax-0.3.16.tar.gz (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting jaxlib<=0.3.16\n","  Downloading jaxlib-0.3.15-cp310-none-manylinux2014_x86_64.whl (72.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from jax<=0.3.16) (1.4.0)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from jax<=0.3.16) (1.23.5)\n","Requirement already satisfied: opt_einsum in /usr/local/lib/python3.10/dist-packages (from jax<=0.3.16) (3.3.0)\n","Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.10/dist-packages (from jax<=0.3.16) (1.11.3)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from jax<=0.3.16) (4.5.0)\n","Requirement already satisfied: etils[epath] in /usr/local/lib/python3.10/dist-packages (from jax<=0.3.16) (1.5.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath]->jax<=0.3.16) (2023.6.0)\n","Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath]->jax<=0.3.16) (6.1.0)\n","Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath]->jax<=0.3.16) (3.17.0)\n","Building wheels for collected packages: jax\n","  Building wheel for jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for jax: filename=jax-0.3.16-py3-none-any.whl size=1197696 sha256=d0a75ede01c8329239a0821834643eae7bbfb91d987f2c65e724467e65f56f64\n","  Stored in directory: /root/.cache/pip/wheels/38/e9/17/7a23533118d1c48ecf3a242929aee566324e2d891eda32e327\n","Successfully built jax\n","Installing collected packages: jaxlib, jax\n","  Attempting uninstall: jaxlib\n","    Found existing installation: jaxlib 0.4.16+cuda11.cudnn86\n","    Uninstalling jaxlib-0.4.16+cuda11.cudnn86:\n","      Successfully uninstalled jaxlib-0.4.16+cuda11.cudnn86\n","  Attempting uninstall: jax\n","    Found existing installation: jax 0.4.16\n","    Uninstalling jax-0.4.16:\n","      Successfully uninstalled jax-0.4.16\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","chex 0.1.7 requires jax>=0.4.6, but you have jax 0.3.16 which is incompatible.\n","flax 0.7.4 requires jax>=0.4.2, but you have jax 0.3.16 which is incompatible.\n","orbax-checkpoint 0.4.1 requires jax>=0.4.9, but you have jax 0.3.16 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed jax-0.3.16 jaxlib-0.3.15\n","Collecting torchtext==0.6\n","  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6) (4.66.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6) (2.31.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6) (2.1.0+cu118)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6) (1.23.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6) (1.16.0)\n","Collecting sentencepiece (from torchtext==0.6)\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6) (2023.7.22)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (3.12.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6) (2.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchtext==0.6) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtext==0.6) (1.3.0)\n","Installing collected packages: sentencepiece, torchtext\n","  Attempting uninstall: torchtext\n","    Found existing installation: torchtext 0.16.0\n","    Uninstalling torchtext-0.16.0:\n","      Successfully uninstalled torchtext-0.16.0\n","Successfully installed sentencepiece-0.1.99 torchtext-0.6.0\n","Collecting datasets\n","  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Collecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.17.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.4)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Installing collected packages: dill, multiprocess, datasets\n","Successfully installed datasets-2.14.5 dill-0.3.7 multiprocess-0.70.15\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","\n","from torchtext import data\n","from torchtext import datasets\n","\n","from transformers import BertTokenizer, BertModel, DistilBertForTokenClassification\n","\n","import numpy as np\n","\n","import time\n","import random\n","import functools\n","from datasets import load_dataset\n","from datasets import concatenate_datasets\n","from datasets import DatasetDict\n","from datasets import Dataset\n","\n","from tqdm.notebook import tqdm\n","\n","import pandas as pd\n","import itertools\n","import re\n","from pprint import pprint\n","\n","import os\n","\n","from chord_tokenizer import ChordTokenizer"],"metadata":{"id":"QiDxn5DMa932","executionInfo":{"status":"ok","timestamp":1697901945812,"user_tz":-180,"elapsed":6648,"user":{"displayName":"Amit Tal","userId":"10134168706322622051"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["class my_tokenizer():\n","    @classmethod\n","    def detokenize(cls, chord_idx):\n","        if chord_idx == 0:\n","          return \"\"\n","        else:\n","          return \"A\"\n","    @classmethod\n","    def tokenize(cls, chord):\n","        #TODO: check handling of o (diminished) and half-diminished symbols\n","        poss_chords_combs_num = 142\n","        slash_ind = -1 #by default - empty\n","        #root - first 2 letters in the chord\n","        if(len(chord) == 1):\n","          root = chord[0].lower()\n","        elif chord[1]=='#' or chord[1]=='b':\n","          root = chord[0:2].lower()\n","          #handle case of 'duplicate notes'\n","          if root==\"db\":\n","            root = \"c#\"\n","          elif root==\"d#\":\n","            root = \"eb\"\n","          elif root==\"gb\":\n","            root = \"f#\"\n","          elif root==\"g#\":\n","            root = \"ab\"\n","          elif root==\"a#\":\n","            root = \"bb\"\n","        else:\n","          root = chord[0].lower()\n","\n","        #convert root from string to num\n","        if root=='a':\n","          root_ind = 0\n","        elif root=='bb':\n","          root_ind = 1\n","        elif root=='b':\n","          root_ind = 2\n","        elif root=='c':\n","          root_ind = 3\n","        elif root=='c#':\n","          root_ind = 4\n","        elif root=='d':\n","          root_ind = 5\n","        elif root=='eb':\n","          root_ind = 6\n","        elif root=='e':\n","          root_ind = 7\n","        elif root=='f':\n","          root_ind = 8\n","        elif root=='f#':\n","          root_ind = 9\n","        elif root=='g':\n","          root_ind = 10\n","        elif root=='ab':\n","          root_ind = 11\n","        else:\n","          return 0\n","        #bass - /\n","        if len(chord) >= 3 and '/' in chord[-3:]:\n","          slash_ind = chord.find('/')\n","          bass = chord[slash_ind+1:].lower()\n","          if bass==\"db\":\n","            bass = \"c#\"\n","          elif bass==\"d#\":\n","            bass = \"eb\"\n","          elif bass==\"gb\":\n","            bass = \"f#\"\n","          elif bass==\"g#\":\n","            bass = \"ab\"\n","          elif bass==\"a#\":\n","            bass = \"bb\"\n","        else:\n","          bass = root\n","\n","\n","        #convert bass from string to num\n","        if bass=='a':\n","          bass_ind = 0\n","        elif bass=='bb':\n","          bass_ind = 1\n","        elif bass=='b':\n","          bass_ind = 2\n","        elif bass=='c':\n","          bass_ind = 3\n","        elif bass=='c#':\n","          bass_ind = 4\n","        elif bass=='d':\n","          bass_ind = 5\n","        elif bass=='eb':\n","          bass_ind = 6\n","        elif bass=='e':\n","          bass_ind = 7\n","        elif bass=='f':\n","          bass_ind = 8\n","        elif bass=='f#':\n","          bass_ind = 9\n","        elif bass=='g':\n","          bass_ind = 10\n","        elif bass=='ab':\n","          bass_ind = 11\n","        else:\n","          return 0\n","        # return bass_ind * 12 + root_ind * (12 ** 2)\n","\n","        #type\n","        if len(chord) == 1:\n","          chord_type = ''\n","        elif slash_ind==-1:\n","          chord_type = chord[len(root):]\n","        else:\n","          chord_type = chord[len(root):slash_ind]\n","        if chord_type == '':\n","            type_ind = 0\n","        elif chord_type == '9':\n","            type_ind = 1\n","        elif chord_type == '11#':\n","            type_ind = 2\n","        elif chord_type == '13':\n","            type_ind = 3\n","        elif chord_type == '911#':\n","            type_ind = 4\n","        elif chord_type == '913':\n","            type_ind = 5\n","        elif chord_type == '11#13':\n","            type_ind = 6\n","        elif chord_type == '911#13':\n","            type_ind = 7\n","        elif chord_type == 'MAJ79' or chord_type == 'M9' or \\\n","        chord_type == 'M79' or chord_type == 'maj79' or chord_type == 'Maj79':\n","            type_ind = 8\n","        elif chord_type == 'MAJ711#' or chord_type == 'M11#' or \\\n","        chord_type == 'M711#' or chord_type == 'maj711#' or chord_type == 'Maj711#':\n","            type_ind = 9\n","        elif chord_type == 'MAJ713' or chord_type == 'M13' or \\\n","        chord_type == 'M713' or chord_type == 'maj713' or chord_type == 'Maj713':\n","            type_ind = 10\n","        elif chord_type == 'MAJ7911#' or chord_type == 'M911#' or \\\n","        chord_type == 'M7911#' or chord_type == 'maj7911#' or chord_type == 'Maj7911#':\n","            type_ind = 11\n","        elif chord_type == 'MAJ7913' or chord_type == 'M913' or \\\n","        chord_type == 'M7913' or chord_type == 'maj7913' or chord_type == 'Maj7913':\n","            type_ind = 12\n","        elif chord_type == 'MAJ711#13' or chord_type == 'M11#13' or \\\n","        chord_type == 'M711#13' or chord_type == 'maj711#13' or chord_type == 'Maj711#13':\n","            type_ind = 13\n","        elif chord_type == 'MAJ7911#13' or chord_type == 'M911#13' or \\\n","        chord_type == 'M7911#13' or chord_type == 'maj7911#13' or chord_type == 'Maj7911#13':\n","            type_ind = 14\n","        elif chord_type == 'm':\n","            type_ind = 15\n","        elif chord_type == 'm9':\n","            type_ind = 16\n","        elif chord_type == 'm11':\n","            type_ind = 17\n","        elif chord_type == 'm13':\n","            type_ind = 18\n","        elif chord_type == 'm911':\n","            type_ind = 19\n","        elif chord_type == 'm913':\n","            type_ind = 20\n","        elif chord_type == 'm1113':\n","            type_ind = 21\n","        elif chord_type == 'm91113':\n","            type_ind = 22\n","        elif chord_type == 'm79':\n","            type_ind = 23\n","        elif chord_type == 'm711':\n","            type_ind = 24\n","        elif chord_type == 'm713':\n","            type_ind = 25\n","        elif chord_type == 'm7911':\n","            type_ind = 26\n","        elif chord_type == 'm7913':\n","            type_ind = 27\n","        elif chord_type == 'm71113':\n","            type_ind = 28\n","        elif chord_type == 'm791113':\n","            type_ind = 29\n","        elif chord_type == '7':\n","            type_ind = 30\n","        elif chord_type == '79b':\n","            type_ind = 31\n","        elif chord_type == '79':\n","            type_ind = 32\n","        elif chord_type == '79#':\n","            type_ind = 33\n","        elif chord_type == '711#':\n","            type_ind = 34\n","        elif chord_type == '713b':\n","            type_ind = 35\n","        elif chord_type == '79b11#':\n","            type_ind = 36\n","        elif chord_type == '7911#':\n","            type_ind = 37\n","        elif chord_type == '79#11#':\n","            type_ind = 38\n","        elif chord_type == '79b13b':\n","            type_ind = 39\n","        elif chord_type == '7913b':\n","            type_ind = 40\n","        elif chord_type == '79#13b':\n","            type_ind = 41\n","        elif chord_type == '711#13b':\n","            type_ind = 42\n","        elif chord_type == '79b11#13b':\n","            type_ind = 43\n","        elif chord_type == '7911#13b':\n","            type_ind = 44\n","        elif chord_type == '79#11#13b':\n","            type_ind = 45\n","        elif chord_type == '6':\n","            type_ind = 46\n","        elif chord_type == '69':\n","            type_ind = 47\n","        elif chord_type == '611#':\n","            type_ind = 48\n","        elif chord_type == '6911#':\n","            type_ind = 49\n","        elif chord_type == 'm6':\n","            type_ind = 50\n","        elif chord_type == 'm69':\n","            type_ind = 51\n","        elif chord_type == 'm611':\n","            type_ind = 52\n","        elif chord_type == 'm6911':\n","            type_ind = 53\n","        elif chord_type == 'MAJ7' or chord_type == 'M' or \\\n","        chord_type == 'M7' or chord_type == 'maj7' or chord_type == 'Maj7':\n","            type_ind = 54\n","        elif chord_type == 'm7':\n","            type_ind = 55\n","        elif chord_type == 'Aug' or chord_type == 'aug' or \\\n","        chord_type == '+':\n","            type_ind = 56\n","        elif chord_type == 'dim' or chord_type == 'd' or \\\n","        chord_type == 'o':\n","            type_ind = 57\n","        elif chord_type == 'dim9' or chord_type == 'd9' or \\\n","        chord_type == 'o9':\n","            type_ind = 58\n","        elif chord_type == 'dim9#' or chord_type == 'd9#' or \\\n","        chord_type == 'o9#':\n","            type_ind = 59\n","        elif chord_type == 'dim11' or chord_type == 'd11' or \\\n","        chord_type == 'o11':\n","            type_ind = 60\n","        elif chord_type == 'dim13' or chord_type == 'd13' or \\\n","        chord_type == 'o13':\n","            type_ind = 61\n","        elif chord_type == 'dim913' or chord_type == 'd913' or \\\n","        chord_type == 'o913':\n","            type_ind = 62\n","        elif chord_type == 'dim9#11' or chord_type == 'd9#11' or \\\n","        chord_type == 'o9#11':\n","            type_ind = 63\n","        elif chord_type == 'dim9#13' or chord_type == 'd9#13' or \\\n","        chord_type == 'o9#13':\n","            type_ind = 64\n","        elif chord_type == 'dim1113' or chord_type == 'd1113' or \\\n","        chord_type == 'o1113':\n","            type_ind = 65\n","        elif chord_type == 'dim91113' or chord_type == 'd91113' or \\\n","        chord_type == 'o91113':\n","            type_ind = 66\n","        elif chord_type == 'dim9#1113' or chord_type == 'd9#1113' or \\\n","        chord_type == 'o9#1113':\n","            type_ind = 67\n","        elif chord_type == 'Aug9' or chord_type == 'aug9' or \\\n","        chord_type == '+9':\n","            type_ind = 68\n","        elif chord_type == 'Aug11' or chord_type == 'aug11' or \\\n","        chord_type == '+11':\n","            type_ind = 69\n","        elif chord_type == 'Aug11#' or chord_type == 'aug11#' or \\\n","        chord_type == '+11#':\n","            type_ind = 70\n","        elif chord_type == 'Aug911' or chord_type == 'aug911' or \\\n","        chord_type == '+911':\n","            type_ind = 71\n","        elif chord_type == 'Aug911#' or chord_type == 'aug911#' or \\\n","        chord_type == '+911#':\n","            type_ind = 72\n","        elif chord_type == 'Aug7' or chord_type == 'aug7' or \\\n","        chord_type == '+7':\n","            type_ind = 73\n","        elif chord_type == 'Aug79' or chord_type == 'aug79' or \\\n","        chord_type == '+79':\n","            type_ind = 74\n","        elif chord_type == 'Aug79b' or chord_type == 'aug79b' or \\\n","        chord_type == '+79b':\n","            type_ind = 75\n","        elif chord_type == 'Aug79#' or chord_type == 'aug79#' or \\\n","        chord_type == '+79#':\n","            type_ind = 76\n","        elif chord_type == 'Aug711' or chord_type == 'aug711' or \\\n","        chord_type == '+711':\n","            type_ind = 77\n","        elif chord_type == 'Aug711#' or chord_type == 'aug711#' or \\\n","        chord_type == '+711#':\n","            type_ind = 78\n","        elif chord_type == 'Aug713' or chord_type == 'aug713' or \\\n","        chord_type == '+713':\n","            type_ind = 79\n","        elif chord_type == 'Aug79b11' or chord_type == 'aug79b11' or \\\n","        chord_type == '+79b11':\n","            type_ind = 80\n","        elif chord_type == 'Aug79b11#' or chord_type == 'aug79b11#' or \\\n","        chord_type == '+79b11#':\n","            type_ind = 81\n","        elif chord_type == 'Aug79b13' or chord_type == 'aug79b13' or \\\n","        chord_type == '+79b13':\n","            type_ind = 82\n","        elif chord_type == 'Aug7911' or chord_type == 'aug7911' or \\\n","        chord_type == '+7911':\n","            type_ind = 83\n","        elif chord_type == 'Aug7911#' or chord_type == 'aug7911#' or \\\n","        chord_type == '+7911#':\n","            type_ind = 84\n","        elif chord_type == 'Aug7913' or chord_type == 'aug7913' or \\\n","        chord_type == '+7913':\n","            type_ind = 85\n","        elif chord_type == 'Aug79#11' or chord_type == 'aug79#11' or \\\n","        chord_type == '+79#11':\n","            type_ind = 86\n","        elif chord_type == 'Aug79#11#' or chord_type == 'aug79#11#' or \\\n","        chord_type == '+79#11#':\n","            type_ind = 87\n","        elif chord_type == 'Aug79#13' or chord_type == 'aug79#13' or \\\n","        chord_type == '+79#13':\n","            type_ind = 88\n","        elif chord_type == 'Aug79b1113' or chord_type == 'aug79b1113' or \\\n","        chord_type == '+79b1113':\n","            type_ind = 89\n","        elif chord_type == 'Aug79b11#13' or chord_type == 'aug79b11#13' or \\\n","        chord_type == '+79b11#13':\n","            type_ind = 90\n","        elif chord_type == 'Aug791113' or chord_type == 'aug791113' or \\\n","        chord_type == '+791113':\n","            type_ind = 91\n","        elif chord_type == 'Aug7911#13' or chord_type == 'aug7911#13' or \\\n","        chord_type == '+7911#13':\n","            type_ind = 92\n","        elif chord_type == 'Aug79#1113' or chord_type == 'aug79#1113' or \\\n","        chord_type == '+79#1113':\n","            type_ind = 93\n","        elif chord_type == 'Aug79#11#13' or chord_type == 'aug79#11#13' or \\\n","        chord_type == '+79#11#13':\n","            type_ind = 94\n","        elif chord_type == 'AugMAJ79b' or chord_type == 'AugM9b' or chord_type == 'AugM79b' \\\n","        or chord_type == 'Augmaj79b' or chord_type == 'AugMaj79b' or chord_type == 'augMAJ79b' \\\n","        or chord_type == 'augM9b' or chord_type == 'augM79b' or chord_type == 'augmaj79b' \\\n","        or chord_type == 'augMaj79b' or chord_type == '+MAJ79b' or chord_type == '+M9b' \\\n","        or chord_type == '+M79b' or chord_type == '+maj79b' or chord_type == '+Maj79b':\n","          type_ind = 95\n","        elif chord_type == 'AugMAJ79' or chord_type == 'AugM9' or chord_type == 'AugM79' \\\n","        or chord_type == 'Augmaj79' or chord_type == 'AugMaj79' or chord_type == 'augMAJ79' \\\n","        or chord_type == 'augM9' or chord_type == 'augM79' or chord_type == 'augmaj79' \\\n","        or chord_type == 'augMaj79' or chord_type == '+MAJ79' or chord_type == '+M9' \\\n","        or chord_type == '+M79' or chord_type == '+maj79' or chord_type == '+Maj79':\n","          type_ind = 96\n","        elif chord_type == 'AugMAJ79#' or chord_type == 'AugM9#' or chord_type == 'AugM79#' \\\n","        or chord_type == 'Augmaj79#' or chord_type == 'AugMaj79#' or chord_type == 'augMAJ79#' \\\n","        or chord_type == 'augM9#' or chord_type == 'augM79#' or chord_type == 'augmaj79#' \\\n","        or chord_type == 'augMaj79#' or chord_type == '+MAJ79#' or chord_type == '+M9#' \\\n","        or chord_type == '+M79#' or chord_type == '+maj79#' or chord_type == '+Maj79#':\n","          type_ind = 97\n","        elif chord_type == 'AugMAJ711' or chord_type == 'AugM11' or chord_type == 'AugM711' \\\n","        or chord_type == 'Augmaj711' or chord_type == 'AugMaj711' or chord_type == 'augMAJ711' \\\n","        or chord_type == 'augM11' or chord_type == 'augM711' or chord_type == 'augmaj711' \\\n","        or chord_type == 'augMaj711' or chord_type == '+MAJ711' or chord_type == '+M11' \\\n","        or chord_type == '+M711' or chord_type == '+maj711' or chord_type == '+Maj711':\n","          type_ind = 98\n","        elif chord_type == 'AugMAJ711#' or chord_type == 'AugM11#' or chord_type == 'AugM711#' \\\n","        or chord_type == 'Augmaj711#' or chord_type == 'AugMaj711#' or chord_type == 'augMAJ711#' \\\n","        or chord_type == 'augM11#' or chord_type == 'augM711#' or chord_type == 'augmaj711#' \\\n","        or chord_type == 'augMaj711#' or chord_type == '+MAJ711#' or chord_type == '+M11#' \\\n","        or chord_type == '+M711#' or chord_type == '+maj711#' or chord_type == '+Maj711#':\n","          type_ind = 99\n","        elif chord_type == 'AugMAJ713' or chord_type == 'AugM13' or chord_type == 'AugM713' \\\n","        or chord_type == 'Augmaj713' or chord_type == 'AugMaj713' or chord_type == 'augMAJ713' \\\n","        or chord_type == 'augM13' or chord_type == 'augM713' or chord_type == 'augmaj713' \\\n","        or chord_type == 'augMaj713' or chord_type == '+MAJ713' or chord_type == '+M13' \\\n","        or chord_type == '+M713' or chord_type == '+maj713' or chord_type == '+Maj713':\n","          type_ind = 100\n","        elif chord_type == 'AugMAJ79b11' or chord_type == 'AugM9b11' or chord_type == 'AugM79b11' \\\n","        or chord_type == 'Augmaj79b11' or chord_type == 'AugMaj79b11' or chord_type == 'augMAJ79b11' \\\n","        or chord_type == 'augM9b11' or chord_type == 'augM79b11' or chord_type == 'augmaj79b11' \\\n","        or chord_type == 'augMaj79b11' or chord_type == '+MAJ79b11' or chord_type == '+M9b11' \\\n","        or chord_type == '+M79b11' or chord_type == '+maj79b11' or chord_type == '+Maj79b11':\n","          type_ind = 101\n","        elif chord_type == 'AugMAJ79b11#' or chord_type == 'AugM9b11#' or chord_type == 'AugM79b11#' \\\n","        or chord_type == 'Augmaj79b11#' or chord_type == 'AugMaj79b11#' or chord_type == 'augMAJ79b11#' \\\n","        or chord_type == 'augM9b11#' or chord_type == 'augM79b11#' or chord_type == 'augmaj79b11#' \\\n","        or chord_type == 'augMaj79b11#' or chord_type == '+MAJ79b11#' or chord_type == '+M9b11#' \\\n","        or chord_type == '+M79b11#' or chord_type == '+maj79b11#' or chord_type == '+Maj79b11#':\n","          type_ind = 102\n","        elif chord_type == 'AugMAJ79b13' or chord_type == 'AugM9b13' or chord_type == 'AugM79b13' \\\n","        or chord_type == 'Augmaj79b13' or chord_type == 'AugMaj79b13' or chord_type == 'augMAJ79b13' \\\n","        or chord_type == 'augM9b13' or chord_type == 'augM79b13' or chord_type == 'augmaj79b13' \\\n","        or chord_type == 'augMaj79b13' or chord_type == '+MAJ79b13' or chord_type == '+M9b13' \\\n","        or chord_type == '+M79b13' or chord_type == '+maj79b13' or chord_type == '+Maj79b13':\n","          type_ind = 103\n","        elif chord_type == 'AugMAJ7911' or chord_type == 'AugM911' or chord_type == 'AugM7911' \\\n","        or chord_type == 'Augmaj7911' or chord_type == 'AugMaj7911' or chord_type == 'augMAJ7911' \\\n","        or chord_type == 'augM911' or chord_type == 'augM7911' or chord_type == 'augmaj7911' \\\n","        or chord_type == 'augMaj7911' or chord_type == '+MAJ7911' or chord_type == '+M911' \\\n","        or chord_type == '+M7911' or chord_type == '+maj7911' or chord_type == '+Maj7911':\n","          type_ind = 104\n","        elif chord_type == 'AugMAJ7911#' or chord_type == 'AugM911#' or chord_type == 'AugM7911#' \\\n","        or chord_type == 'Augmaj7911#' or chord_type == 'AugMaj7911#' or chord_type == 'augMAJ7911#' \\\n","        or chord_type == 'augM911#' or chord_type == 'augM7911#' or chord_type == 'augmaj7911#' \\\n","        or chord_type == 'augMaj7911#' or chord_type == '+MAJ7911#' or chord_type == '+M911#' \\\n","        or chord_type == '+M7911#' or chord_type == '+maj7911#' or chord_type == '+Maj7911#':\n","          type_ind = 105\n","        elif chord_type == 'AugMAJ7913' or chord_type == 'AugM913' or chord_type == 'AugM7913' \\\n","        or chord_type == 'Augmaj7913' or chord_type == 'AugMaj7913' or chord_type == 'augMAJ7913' \\\n","        or chord_type == 'augM913' or chord_type == 'augM7913' or chord_type == 'augmaj7913' \\\n","        or chord_type == 'augMaj7913' or chord_type == '+MAJ7913' or chord_type == '+M913' \\\n","        or chord_type == '+M7913' or chord_type == '+maj7913' or chord_type == '+Maj7913':\n","          type_ind = 106\n","        elif chord_type == 'AugMAJ79#11' or chord_type == 'AugM9#11' or chord_type == 'AugM79#11' \\\n","        or chord_type == 'Augmaj79#11' or chord_type == 'AugMaj79#11' or chord_type == 'augMAJ79#11' \\\n","        or chord_type == 'augM9#11' or chord_type == 'augM79#11' or chord_type == 'augmaj79#11' \\\n","        or chord_type == 'augMaj79#11' or chord_type == '+MAJ79#11' or chord_type == '+M9#11' \\\n","        or chord_type == '+M79#11' or chord_type == '+maj79#11' or chord_type == '+Maj79#11':\n","          type_ind = 107\n","        elif chord_type == 'AugMAJ79#11#' or chord_type == 'AugM9#11#' or chord_type == 'AugM79#11#' \\\n","        or chord_type == 'Augmaj79#11#' or chord_type == 'AugMaj79#11#' or chord_type == 'augMAJ79#11#' \\\n","        or chord_type == 'augM9#11#' or chord_type == 'augM79#11#' or chord_type == 'augmaj79#11#' \\\n","        or chord_type == 'augMaj79#11#' or chord_type == '+MAJ79#11#' or chord_type == '+M9#11#' \\\n","        or chord_type == '+M79#11#' or chord_type == '+maj79#11#' or chord_type == '+Maj79#11#':\n","          type_ind = 108\n","        elif chord_type == 'AugMAJ79#13' or chord_type == 'AugM9#13' or chord_type == 'AugM79#13' \\\n","        or chord_type == 'Augmaj79#13' or chord_type == 'AugMaj79#13' or chord_type == 'augMAJ79#13' \\\n","        or chord_type == 'augM9#13' or chord_type == 'augM79#13' or chord_type == 'augmaj79#13' \\\n","        or chord_type == 'augMaj79#13' or chord_type == '+MAJ79#13' or chord_type == '+M9#13' \\\n","        or chord_type == '+M79#13' or chord_type == '+maj79#13' or chord_type == '+Maj79#13':\n","          type_ind = 109\n","        elif chord_type == 'AugMAJ79b1113' or chord_type == 'AugM9b1113' or chord_type == 'AugM79b1113' \\\n","        or chord_type == 'Augmaj79b1113' or chord_type == 'AugMaj79b1113' or chord_type == 'augMAJ79b1113' \\\n","        or chord_type == 'augM9b1113' or chord_type == 'augM79b1113' or chord_type == 'augmaj79b1113' \\\n","        or chord_type == 'augMaj79b1113' or chord_type == '+MAJ79b1113' or chord_type == '+M9b1113' \\\n","        or chord_type == '+M79b1113' or chord_type == '+maj79b1113' or chord_type == '+Maj79b1113':\n","          type_ind = 110\n","        elif chord_type == 'AugMAJ79b11#13' or chord_type == 'AugM9b11#13' or chord_type == 'AugM79b11#13' \\\n","        or chord_type == 'Augmaj79b11#13' or chord_type == 'AugMaj79b11#13' or chord_type == 'augMAJ79b11#13' \\\n","        or chord_type == 'augM9b11#13' or chord_type == 'augM79b11#13' or chord_type == 'augmaj79b11#13' \\\n","        or chord_type == 'augMaj79b11#13' or chord_type == '+MAJ79b11#13' or chord_type == '+M9b11#13' \\\n","        or chord_type == '+M79b11#13' or chord_type == '+maj79b11#13' or chord_type == '+Maj79b11#13':\n","          type_ind = 111\n","        elif chord_type == 'AugMAJ791113' or chord_type == 'AugM91113' or chord_type == 'AugM791113' \\\n","        or chord_type == 'Augmaj791113' or chord_type == 'AugMaj791113' or chord_type == 'augMAJ791113' \\\n","        or chord_type == 'augM91113' or chord_type == 'augM791113' or chord_type == 'augmaj791113' \\\n","        or chord_type == 'augMaj791113' or chord_type == '+MAJ791113' or chord_type == '+M91113' \\\n","        or chord_type == '+M791113' or chord_type == '+maj791113' or chord_type == '+Maj791113':\n","          type_ind = 112\n","        elif chord_type == 'AugMAJ7911#13' or chord_type == 'AugM911#13' or chord_type == 'AugM7911#13' \\\n","        or chord_type == 'Augmaj7911#13' or chord_type == 'AugMaj7911#13' or chord_type == 'augMAJ7911#13' \\\n","        or chord_type == 'augM911#13' or chord_type == 'augM7911#13' or chord_type == 'augmaj7911#13' \\\n","        or chord_type == 'augMaj7911#13' or chord_type == '+MAJ7911#13' or chord_type == '+M911#13' \\\n","        or chord_type == '+M7911#13' or chord_type == '+maj7911#13' or chord_type == '+Maj7911#13':\n","          type_ind = 113\n","        elif chord_type == 'AugMAJ79#1113' or chord_type == 'AugM9#1113' or chord_type == 'AugM79#1113' \\\n","        or chord_type == 'Augmaj79#1113' or chord_type == 'AugMaj79#1113' or chord_type == 'augMAJ79#1113' \\\n","        or chord_type == 'augM9#1113' or chord_type == 'augM79#1113' or chord_type == 'augmaj79#1113' \\\n","        or chord_type == 'augMaj79#1113' or chord_type == '+MAJ79#1113' or chord_type == '+M9#1113' \\\n","        or chord_type == '+M79#1113' or chord_type == '+maj79#1113' or chord_type == '+Maj79#1113':\n","          type_ind = 114\n","        elif chord_type == 'AugMAJ79#11#13' or chord_type == 'AugM9#11#13' or chord_type == 'AugM79#11#13' \\\n","        or chord_type == 'Augmaj79#11#13' or chord_type == 'AugMaj79#11#13' or chord_type == 'augMAJ79#11#13' \\\n","        or chord_type == 'augM9#11#13' or chord_type == 'augM79#11#13' or chord_type == 'augmaj79#11#13' \\\n","        or chord_type == 'augMaj79#11#13' or chord_type == '+MAJ79#11#13' or chord_type == '+M9#11#13' \\\n","        or chord_type == '+M79#11#13' or chord_type == '+maj79#11#13' or chord_type == '+Maj79#11#13':\n","          type_ind = 115\n","        elif chord_type == 'AugMAJ7' or chord_type == 'AugM' or chord_type == 'AugM7' \\\n","        or chord_type == 'Augmaj7' or chord_type == 'AugMaj7' or chord_type == 'augMAJ7' \\\n","        or chord_type == 'augM' or chord_type == 'augM7' or chord_type == 'augmaj7' \\\n","        or chord_type == 'augMaj7' or chord_type == '+MAJ7' or chord_type == '+M' \\\n","        or chord_type == '+M7' or chord_type == '+maj7' or chord_type == '+Maj7':\n","          type_ind = 116\n","        elif chord_type == 'dim7' or chord_type == 'd7' or \\\n","        chord_type == 'o7':\n","            type_ind = 117\n","        elif chord_type == 'dim79' or chord_type == 'd79' or \\\n","        chord_type == 'o79':\n","            type_ind = 118\n","        elif chord_type == 'dim79#' or chord_type == 'd79#' or \\\n","        chord_type == 'o79#':\n","            type_ind = 119\n","        elif chord_type == 'dim711' or chord_type == 'd711' or \\\n","        chord_type == 'o711':\n","            type_ind = 120\n","        elif chord_type == 'dim7911' or chord_type == 'd911' or \\\n","        chord_type == 'o911':\n","            type_ind = 121\n","        elif chord_type == 'dim79#11' or chord_type == 'd79#11' or \\\n","        chord_type == 'o79#11':\n","            type_ind = 122\n","        elif chord_type == 'dim7913' or chord_type == 'd7913' or \\\n","        chord_type == 'o7913':\n","            type_ind = 123\n","        elif chord_type == 'dim79#13' or chord_type == 'd79#13' or \\\n","        chord_type == 'o79#13':\n","            type_ind = 124\n","        elif chord_type == 'dim71113' or chord_type == 'd71113' or \\\n","        chord_type == 'o71113':\n","            type_ind = 125\n","        elif chord_type == 'dim791113' or chord_type == 'd791113' or \\\n","        chord_type == 'o791113':\n","            type_ind = 126\n","        elif chord_type == 'dim79#1113' or chord_type == 'd9#71113' or \\\n","        chord_type == 'o79#1113':\n","            type_ind = 127\n","        #TODO: take care of half-diminished symbol\n","        elif chord_type == 'm7b5':\n","            type_ind = 128\n","        elif chord_type == 'm7b59':\n","            type_ind = 129\n","        elif chord_type == 'm7b59#':\n","            type_ind = 130\n","        elif chord_type == 'm7b511':\n","            type_ind = 131\n","        elif chord_type == 'm7b513b':\n","            type_ind = 132\n","        elif chord_type == 'm7b5911':\n","            type_ind = 133\n","        elif chord_type == 'm7b5913b':\n","            type_ind = 134\n","        elif chord_type == 'm7b59#11':\n","            type_ind = 135\n","        elif chord_type == 'm7b59#13b':\n","            type_ind = 136\n","        elif chord_type == 'm7b51113b':\n","            type_ind = 137\n","        elif chord_type == 'm7b591113b':\n","            type_ind = 138\n","        elif chord_type == 'm7b59#1113':\n","            type_ind = 139\n","        elif chord_type == 'sus4' or chord_type == '4':\n","            type_ind = 140\n","        elif chord_type == 'sus2' or chord_type == '2':\n","            type_ind = 141\n","        else:\n","          return 0\n","        return sum([((12 ** i) * val) for i,val in enumerate([root_ind, bass_ind, type_ind])]) + 1\n","\n","    def encode(input, max_input_length):\n","        ans = {'input_ids':[], 'token_type_ids':[], 'attention_mask':[]}\n","        for chords in input:\n","            chords = chords.split()\n","            tokens = [0]\n","            i=2\n","            for chord in chords:\n","                i+=1\n","                tokens.append(my_tokenizer.tokenize(chord))\n","            tokens.append(0)\n","            padding = [0] * (max_input_length - i)\n","            ans['input_ids'].append(tokens + padding)\n","            ans['token_type_ids'].append([0 for _ in range(len(tokens))] + padding)\n","            ans['attention_mask'].append([1 for _ in range(len(tokens))] + padding)\n","        return ans\n","\n"],"metadata":{"id":"AoYFE39na98d","executionInfo":{"status":"ok","timestamp":1697901946040,"user_tz":-180,"elapsed":237,"user":{"displayName":"Amit Tal","userId":"10134168706322622051"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","\n","init_token = tokenizer.cls_token\n","pad_token = tokenizer.pad_token\n","unk_token = tokenizer.unk_token\n","\n","init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n","pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n","unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n","\n","max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n","print(init_token_idx)\n","print(pad_token_idx)\n","print(unk_token_idx)\n","print(max_input_length)\n","\n","\n"],"metadata":{"id":"XKGE_v1rLZqI","colab":{"base_uri":"https://localhost:8080/","height":218,"referenced_widgets":["4c0391611c9e40368966a36612007645","6aadfee69e184d459baa97f159e6e925","be244f788f394dd4b39f84fc3464c2f0","b009753ec1d647789211ff24e5f86754","a96d854d3c224c6686f6a4506d6193e0","3132306d83b94684b3a723255f99740c","4179b1bb81ea406a95913a4c3bc14d08","5537e6d2df0841169b5e7b6a4d5af8b6","4448011001bf4af89ae30ed69ba910c9","76a74c4d60584b90bb4373bb03a7278b","e6f14c719af24f0ab7a217db5107c008","9d88969feeb24d9cab1b851a27164c90","534daa3ab27248db8e9ef586f92bfc9a","31af7f9fa05f4631a408386c747d27ce","dc0dd1ac32f7431e8a2f3214fba5bcb8","e0dd1c1cf0cc4717b187b43e0b1ef847","6e52516bab7242a08eeb562eea04ca24","278532f611834698ab4546332c3a86de","74537096b29549a59d99487c5ca667a5","b1b57316e5924d8281aa965ff9a126d1","4b11bdbd8c70438daebbc10bb48cac77","99695df336e441f2ab944c4f01593dc7","33017f1765d643678016d0738b4b528b","cde790f3d03c4abdb26e5af6818863b7","c6573c1038234679949049c4266d42da","4d0150ede6e44cbf94d0b3643d1495a9","65fc45284b2448449831ce70341d18e9","4e9186b5fab44cb88d91d0749b8bb3ce","13ec26994bd84bac9b6083751ada8910","0fb54aaaccfd4f7d94da9c4d6efdd42f","fddccf6150eb48469b1b48094d91c46f","3084fab0aa25451f9c5072c23d4a3968","4bea1eccf4db453d8151ace93ea1a2bc","cb7398583a5a45e08ae481c2ed094632","7bacf61593bf40a1a2d1cf2d530a86d5","46a081d21fc24a7580ca2c8cb555edc4","2a2267cca7914a699870ffafee8585c7","f25eb2b895c244ba829ee939d3a6f1f9","23dcf4fad2764906a22850466b0e4c40","358e6ae07d02446cb233260ee605dd62","ba6c17a554124c8d8a3458fcaf0f57fa","e09e6ec9fbf542ab99fca107668e19b3","35a1ddfc71834c93ac0e9cdf286b5f69","da7c61e5eda141b687b892a16cc012c1"]},"executionInfo":{"status":"ok","timestamp":1697901947380,"user_tz":-180,"elapsed":774,"user":{"displayName":"Amit Tal","userId":"10134168706322622051"}},"outputId":"66aca513-7f8a-4c6c-a908-1220cf138a79"},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c0391611c9e40368966a36612007645"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d88969feeb24d9cab1b851a27164c90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33017f1765d643678016d0738b4b528b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb7398583a5a45e08ae481c2ed094632"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["101\n","0\n","100\n","512\n"]}]},{"cell_type":"code","source":["if os.path.isfile('dataset/chords_and_lyrics_en.pkl'):\n","  english_chords = pd.read_pickle('dataset/chords_and_lyrics_en.pkl')\n","else:\n","  data = pd.read_pickle('dataset/chords_and_lyrics.pkl')\n","  english_chords = data[data['lang'] == 'en']\n","  english_chords.to_pickle('dataset/chords_and_lyrics_en.pkl')\n","  english_chords.to_csv('dataset/chords_and_lyrics_en.csv')\n"],"metadata":{"id":"f3PWTUM7t-Bj","executionInfo":{"status":"ok","timestamp":1697901950718,"user_tz":-180,"elapsed":3340,"user":{"displayName":"Amit Tal","userId":"10134168706322622051"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def find_ind_word_to_chord_ind(chord_ind, line):\n","  count = 0\n","  letter_ind = min(chord_ind, len(line)-1)\n","  is_space = line[letter_ind].isspace()\n","  if is_space:\n","    count +=1\n","  for i in range(letter_ind,-1,-1):\n","    if is_space == False and line[i].isspace() == True:\n","      count +=1\n","    is_space = line[i].isspace()\n","  return count\n","\n"],"metadata":{"id":"-hBbH_LVOmn8","executionInfo":{"status":"ok","timestamp":1697901950719,"user_tz":-180,"elapsed":3,"user":{"displayName":"Amit Tal","userId":"10134168706322622051"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["lyrics = english_chords[\"lyrics\"]\n","chords = english_chords[\"chords\"]\n","annotated_lyrics = []\n","annotated_chords = []\n","annotated_lyrics_input_ids = []\n","annotated_lyrics_token_type_ids = []\n","annotated_lyrics_attention_mask = []\n","annotated_chords_chords_id = []\n","data = []\n","for i in range(len(lyrics)):\n","  row = {}\n","  if i % 1000 == 0:\n","    print(i)\n","  if i not in lyrics or i not in chords:\n","    continue\n","  lyric = lyrics[i]\n","  chord = chords[i]\n","  text = \"\"\n","  for j in range(len(lyric)):\n","    text += \" @ \"                 # new line sign\n","    if 2*j not in lyric:\n","      continue\n","    line = lyric[2*j]\n","    if len(line)==0:\n","      continue\n","    line = line.replace('\\n', '')\n","    line = line.replace('\\t', '')\n","    words_in_line = line.split()\n","    if len(words_in_line) == 0:\n","      continue\n","    text += line\n","\n","  tokenized_text = tokenizer(text,\n","                             padding='max_length',\n","                             truncation=True)\n","\n","  tokenized_chords = list(np.zeros(len(tokenized_text['input_ids']), int))\n","  annotated_lyrics_input_ids.append(tokenized_text['input_ids'])\n","  annotated_lyrics_token_type_ids.append(tokenized_text['token_type_ids'])\n","  annotated_lyrics_attention_mask.append(tokenized_text['attention_mask'])\n","  row['input_ids'] = tokenized_text['input_ids']\n","  row['token_type_ids'] = tokenized_text['token_type_ids']\n","  row['attention_mask'] = tokenized_text['attention_mask']\n","\n","  words_in_text = text.split()\n","  start_from_ind = 1\n","  for j in range(len(lyric)):\n","    if 2*j not in lyric:\n","      continue\n","    line = lyric[2*j]\n","    if len(line)==0:\n","      continue\n","    line = line.replace('\\n', '')\n","    line = line.replace('\\t', '')\n","    words_in_line = line.split()\n","    tokenized_line = tokenizer(line)\n","    if len(words_in_line) == 0:\n","      continue\n","\n","    if (2*j-1) in chord:\n","      line_chords = chord[2*j-1]\n","      line_chords = line_chords.replace('\\n', '')\n","      line_chords = line_chords.replace('\\t', '')\n","      chords_in_line = line_chords.split()\n","\n","      for chord_in_line in chords_in_line:\n","        chord_ind = line_chords.find(chord_in_line)\n","        #word = find_word_to_chord_ind(chord_ind, line)\n","        #word_ind = words_in_line.index(word)\n","        word_ind = find_ind_word_to_chord_ind(chord_ind, line)\n","        ind = start_from_ind + word_ind\n","        if ind < max_input_length:\n","          tokenized_chords[ind] = my_tokenizer.tokenize(chord_in_line)\n","    start_from_ind += len(tokenized_line['input_ids'])-2 + 1\n","    #start_from_ind += len(words_in_line) +1\n","##############################################################\n","#  current_chord = 0\n","#  for ind in range(len(tokenized_chords)):\n","#    if tokenized_chords[ind] == 0:\n","#      tokenized_chords[ind] = current_chord\n","#    else:\n","#      current_chord = tokenized_chords[ind]\n","#  tokenized_chords = [tokenized_chords[i] * tokenized_text['attention_mask'][i] for i in range(len(tokenized_chords))]\n","##############################################################\n","  annotated_lyrics.append(tokenized_text)\n","  annotated_chords.append(tokenized_chords)\n","  annotated_chords_chords_id.append(tokenized_chords)\n","  row['labels'] = tokenized_chords\n","  data.append(row)\n","\n","print(annotated_lyrics[0])\n","print(annotated_chords[0])\n","print(annotated_lyrics[0]['input_ids'])\n","print(annotated_lyrics[0]['token_type_ids'])\n","print(annotated_lyrics[0]['attention_mask'])\n","\n"],"metadata":{"id":"tZBHsAmbD8J4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697902319041,"user_tz":-180,"elapsed":368324,"user":{"displayName":"Amit Tal","userId":"10134168706322622051"}},"outputId":"b6a425e9-dd54-4f9d-91cb-9307f0fa401e"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","1000\n","2000\n","3000\n","4000\n","5000\n","6000\n","7000\n","8000\n","9000\n","10000\n","11000\n","12000\n","13000\n","14000\n","15000\n","16000\n","17000\n","18000\n","19000\n","20000\n","21000\n","22000\n","23000\n","24000\n","25000\n","26000\n","27000\n","28000\n","29000\n","30000\n","31000\n","32000\n","33000\n","34000\n","35000\n","36000\n","37000\n","38000\n","39000\n","40000\n","41000\n","42000\n","43000\n","44000\n","45000\n","46000\n","47000\n","48000\n","49000\n","50000\n","51000\n","52000\n","53000\n","54000\n","55000\n","56000\n","57000\n","58000\n","59000\n","60000\n","61000\n","62000\n","63000\n","64000\n","65000\n","66000\n","67000\n","68000\n","69000\n","70000\n","71000\n","72000\n","73000\n","74000\n","75000\n","76000\n","77000\n","78000\n","79000\n","80000\n","{'input_ids': [101, 1030, 6178, 2080, 2006, 3822, 10424, 3388, 1030, 7893, 1015, 1024, 1030, 2079, 2017, 2293, 1996, 4542, 1010, 2515, 2009, 2191, 2017, 3153, 1030, 2043, 2017, 1005, 2128, 7144, 2007, 2115, 2814, 2012, 1037, 2283, 1029, 1030, 2054, 1005, 1055, 2115, 5440, 2299, 1010, 2515, 2009, 2191, 2017, 2868, 1029, 1030, 2079, 2017, 2228, 1997, 2033, 1029, 1030, 3653, 1011, 7165, 1030, 2043, 2017, 2485, 2115, 2159, 1010, 2425, 2033, 2054, 2024, 2017, 3959, 2378, 1005, 1029, 1030, 2673, 1010, 1045, 1011, 1011, 10587, 2113, 2009, 2035, 1030, 1030, 1045, 1005, 1040, 5247, 2702, 4595, 2847, 1998, 2702, 4595, 2062, 1030, 2821, 2065, 2008, 1005, 1055, 2054, 2009, 3138, 2000, 4553, 2008, 4086, 2540, 1997, 6737, 1030, 1998, 1045, 2453, 2196, 2131, 2045, 1010, 2021, 1045, 1005, 1049, 6069, 3046, 1030, 2065, 2009, 1005, 1055, 2702, 4595, 2847, 2030, 1996, 2717, 1997, 2026, 2166, 1030, 1045, 1005, 1049, 6069, 2293, 2017, 1006, 1051, 9541, 2232, 1051, 11631, 1011, 1051, 9541, 2232, 1051, 11631, 1011, 1051, 11631, 1007, 1030, 7893, 1016, 1024, 1030, 2079, 2017, 3335, 1996, 2346, 2008, 2017, 3473, 2039, 2006, 1029, 1030, 2106, 2017, 2131, 2115, 2690, 2171, 2013, 2115, 13055, 1029, 1030, 2043, 2017, 2228, 2055, 2115, 5091, 2085, 1030, 2079, 2017, 2228, 1997, 2033, 1029, 1030, 3653, 1011, 7165, 1030, 2043, 2017, 2485, 2115, 2159, 1010, 2425, 2033, 2054, 2024, 2017, 3959, 2378, 1005, 1029, 1030, 2673, 1010, 1045, 1011, 1011, 10587, 2113, 2009, 2035, 1030, 1030, 1045, 1005, 1040, 5247, 2702, 4595, 2847, 1998, 2702, 4595, 2062, 1030, 2821, 2065, 2008, 1005, 1055, 2054, 2009, 3138, 2000, 4553, 2008, 4086, 2540, 1997, 6737, 1030, 1998, 1045, 2453, 2196, 2131, 2045, 1010, 2021, 1045, 1005, 1049, 6069, 3046, 1030, 2065, 2009, 1005, 1055, 2702, 4595, 2847, 2030, 1996, 2717, 1997, 2026, 2166, 1030, 1045, 1005, 1049, 6069, 2293, 2017, 1006, 1051, 9541, 2232, 1051, 11631, 1011, 1051, 9541, 2232, 1051, 11631, 1011, 1051, 11631, 1007, 1030, 1045, 1005, 1049, 6069, 2293, 2017, 1006, 1051, 9541, 2232, 1051, 11631, 1011, 1051, 9541, 2232, 1051, 11631, 1011, 1051, 11631, 1007, 1030, 1030, 1051, 11631, 1010, 2215, 1996, 2204, 1998, 1996, 2919, 1010, 2673, 1999, 2090, 1030, 1051, 11631, 1010, 10657, 9526, 2026, 10628, 1030, 1051, 11631, 1010, 3398, 1030, 1030, 1045, 1005, 1040, 5247, 2702, 4595, 2847, 1998, 2702, 4595, 2062, 1030, 2821, 2065, 2008, 1005, 1055, 2054, 2009, 3138, 2000, 4553, 2008, 4086, 2540, 1997, 6737, 1006, 4086, 2540, 1997, 6737, 1007, 1030, 1998, 1045, 2453, 2196, 2131, 2045, 1010, 2021, 1045, 1005, 1049, 6069, 3046, 1006, 3398, 1007, 1030, 2065, 2009, 1005, 1055, 2702, 4595, 2847, 2030, 1996, 2717, 1997, 2026, 2166, 1030, 1045, 1005, 1049, 6069, 2293, 2017, 1006, 1051, 9541, 2232, 1051, 11631, 1011, 1051, 9541, 2232, 1051, 11631, 1011, 1051, 11631, 1007, 1030, 1045, 1005, 1049, 6069, 2293, 2017, 1006, 1051, 9541, 2232, 1051, 11631, 1011, 1051, 9541, 2232, 1007, 3398, 1006, 1051, 11631, 1011, 1051, 11631, 1007, 1030, 2345, 1030, 1030, 1030, 1030, 1030, 1030, 1030, 1030, 1030, 1030, 1030, 1030, 1030, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 131, 0, 0, 35, 0, 0, 0, 0, 0, 40, 0, 0, 0, 0, 131, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 131, 0, 35, 0, 0, 0, 0, 0, 40, 0, 0, 0, 0, 0, 0, 0, 0, 131, 0, 0, 0, 0, 0, 0, 0, 0, 131, 0, 2252, 0, 0, 40, 0, 0, 0, 0, 0, 0, 0, 0, 131, 0, 2252, 0, 0, 0, 40, 0, 0, 0, 0, 0, 131, 0, 2252, 0, 40, 0, 0, 0, 0, 0, 0, 0, 131, 0, 0, 2252, 0, 0, 0, 40, 0, 0, 0, 0, 0, 0, 0, 0, 131, 0, 2252, 0, 0, 40, 0, 0, 0, 0, 0, 0, 0, 0, 131, 0, 2252, 0, 0, 40, 0, 0, 0, 0, 0, 0, 0, 0, 131, 0, 2252, 40, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 131, 0, 0, 35, 0, 0, 0, 0, 0, 40, 0, 0, 0, 0, 0, 131, 0, 0, 0, 0, 0, 0, 0, 131, 0, 0, 35, 0, 0, 40, 0, 0, 0, 0, 131, 0, 0, 0, 0, 0, 0, 0, 0, 131, 0, 2252, 0, 0, 40, 0, 0, 0, 0, 0, 0, 0, 0, 131, 0, 2252, 0, 0, 0, 40, 0, 0, 0, 0, 0, 131, 0, 2252, 0, 40, 0, 0, 0, 0, 0, 0, 0, 131, 0, 0, 2252, 0, 0, 0, 40, 0, 0, 0, 0, 0, 0, 0, 0, 131, 0, 2252, 0, 0, 40, 0, 0, 0, 0, 0, 0, 0, 0, 131, 0, 2252, 0, 0, 40, 0, 0, 0, 0, 0, 0, 0, 0, 131, 0, 2252, 40, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 131, 0, 2252, 40, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2187, 0, 0, 40, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2187, 0, 40, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 131, 0, 2252, 0, 40, 0, 0, 0, 0, 0, 0, 0, 131, 0, 0, 2252, 0, 0, 0, 40, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 131, 0, 2252, 0, 0, 40, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 131, 0, 2252, 0, 0, 40, 0, 0, 0, 0, 0, 0, 0, 0, 131, 0, 2252, 40, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 131, 0, 2252, 40, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","[101, 1030, 6178, 2080, 2006, 3822, 10424, 3388, 1030, 7893, 1015, 1024, 1030, 2079, 2017, 2293, 1996, 4542, 1010, 2515, 2009, 2191, 2017, 3153, 1030, 2043, 2017, 1005, 2128, 7144, 2007, 2115, 2814, 2012, 1037, 2283, 1029, 1030, 2054, 1005, 1055, 2115, 5440, 2299, 1010, 2515, 2009, 2191, 2017, 2868, 1029, 1030, 2079, 2017, 2228, 1997, 2033, 1029, 1030, 3653, 1011, 7165, 1030, 2043, 2017, 2485, 2115, 2159, 1010, 2425, 2033, 2054, 2024, 2017, 3959, 2378, 1005, 1029, 1030, 2673, 1010, 1045, 1011, 1011, 10587, 2113, 2009, 2035, 1030, 1030, 1045, 1005, 1040, 5247, 2702, 4595, 2847, 1998, 2702, 4595, 2062, 1030, 2821, 2065, 2008, 1005, 1055, 2054, 2009, 3138, 2000, 4553, 2008, 4086, 2540, 1997, 6737, 1030, 1998, 1045, 2453, 2196, 2131, 2045, 1010, 2021, 1045, 1005, 1049, 6069, 3046, 1030, 2065, 2009, 1005, 1055, 2702, 4595, 2847, 2030, 1996, 2717, 1997, 2026, 2166, 1030, 1045, 1005, 1049, 6069, 2293, 2017, 1006, 1051, 9541, 2232, 1051, 11631, 1011, 1051, 9541, 2232, 1051, 11631, 1011, 1051, 11631, 1007, 1030, 7893, 1016, 1024, 1030, 2079, 2017, 3335, 1996, 2346, 2008, 2017, 3473, 2039, 2006, 1029, 1030, 2106, 2017, 2131, 2115, 2690, 2171, 2013, 2115, 13055, 1029, 1030, 2043, 2017, 2228, 2055, 2115, 5091, 2085, 1030, 2079, 2017, 2228, 1997, 2033, 1029, 1030, 3653, 1011, 7165, 1030, 2043, 2017, 2485, 2115, 2159, 1010, 2425, 2033, 2054, 2024, 2017, 3959, 2378, 1005, 1029, 1030, 2673, 1010, 1045, 1011, 1011, 10587, 2113, 2009, 2035, 1030, 1030, 1045, 1005, 1040, 5247, 2702, 4595, 2847, 1998, 2702, 4595, 2062, 1030, 2821, 2065, 2008, 1005, 1055, 2054, 2009, 3138, 2000, 4553, 2008, 4086, 2540, 1997, 6737, 1030, 1998, 1045, 2453, 2196, 2131, 2045, 1010, 2021, 1045, 1005, 1049, 6069, 3046, 1030, 2065, 2009, 1005, 1055, 2702, 4595, 2847, 2030, 1996, 2717, 1997, 2026, 2166, 1030, 1045, 1005, 1049, 6069, 2293, 2017, 1006, 1051, 9541, 2232, 1051, 11631, 1011, 1051, 9541, 2232, 1051, 11631, 1011, 1051, 11631, 1007, 1030, 1045, 1005, 1049, 6069, 2293, 2017, 1006, 1051, 9541, 2232, 1051, 11631, 1011, 1051, 9541, 2232, 1051, 11631, 1011, 1051, 11631, 1007, 1030, 1030, 1051, 11631, 1010, 2215, 1996, 2204, 1998, 1996, 2919, 1010, 2673, 1999, 2090, 1030, 1051, 11631, 1010, 10657, 9526, 2026, 10628, 1030, 1051, 11631, 1010, 3398, 1030, 1030, 1045, 1005, 1040, 5247, 2702, 4595, 2847, 1998, 2702, 4595, 2062, 1030, 2821, 2065, 2008, 1005, 1055, 2054, 2009, 3138, 2000, 4553, 2008, 4086, 2540, 1997, 6737, 1006, 4086, 2540, 1997, 6737, 1007, 1030, 1998, 1045, 2453, 2196, 2131, 2045, 1010, 2021, 1045, 1005, 1049, 6069, 3046, 1006, 3398, 1007, 1030, 2065, 2009, 1005, 1055, 2702, 4595, 2847, 2030, 1996, 2717, 1997, 2026, 2166, 1030, 1045, 1005, 1049, 6069, 2293, 2017, 1006, 1051, 9541, 2232, 1051, 11631, 1011, 1051, 9541, 2232, 1051, 11631, 1011, 1051, 11631, 1007, 1030, 1045, 1005, 1049, 6069, 2293, 2017, 1006, 1051, 9541, 2232, 1051, 11631, 1011, 1051, 9541, 2232, 1007, 3398, 1006, 1051, 11631, 1011, 1051, 11631, 1007, 1030, 2345, 1030, 1030, 1030, 1030, 1030, 1030, 1030, 1030, 1030, 1030, 1030, 1030, 1030, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"]}]},{"cell_type":"code","source":["dict = {'input_ids': annotated_lyrics_input_ids, 'token_type_ids': annotated_lyrics_token_type_ids, 'attention_mask': annotated_lyrics_attention_mask, 'chords_id': annotated_chords_chords_id}\n","df = pd.DataFrame(dict)\n","df.to_csv('annotated_data.csv')"],"metadata":{"id":"EPL3aOJoxa7Z","executionInfo":{"status":"ok","timestamp":1697902343410,"user_tz":-180,"elapsed":24373,"user":{"displayName":"Amit Tal","userId":"10134168706322622051"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["#DATA = load_dataset(\"csv\", \"annotated_data\")\n","dataset = Dataset.from_pandas(pd.DataFrame(data=data))\n","#DATA = Dataset.from_list(annotated_lyrics_input_ids)\n","dataset = dataset.remove_columns([\"token_type_ids\"])\n","dataset.set_format(\"torch\")\n","dataset = DatasetDict(\n","    train=dataset.shuffle(seed=1111).select(range(int(0.8*len(dataset)))),\n","    val=dataset.shuffle(seed=1111).select(range(int(0.8*len(dataset)), len(dataset)))\n",")\n","print(dataset)\n","print(dataset['train'][0:2])\n","print(len(dataset['train']))\n","print(dataset['val'][0])\n","print(len(dataset['val']))"],"metadata":{"id":"dEgv-gZ86x1Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697902351761,"user_tz":-180,"elapsed":8361,"user":{"displayName":"Amit Tal","userId":"10134168706322622051"}},"outputId":"d6a252b7-a848-4723-f12e-fc6ea714946b"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['input_ids', 'attention_mask', 'labels'],\n","        num_rows: 40708\n","    })\n","    val: Dataset({\n","        features: ['input_ids', 'attention_mask', 'labels'],\n","        num_rows: 10178\n","    })\n","})\n","{'input_ids': tensor([[ 101, 1030, 1030,  ...,    0,    0,    0],\n","        [ 101, 1030, 1030,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  0,  27,   0,  ...,   0,   0,   0],\n","        [  0,   0, 171,  ...,   0,   0,   0]])}\n","40708\n","{'input_ids': tensor([  101,  1030,  1030,  7893,  1015,  1024,  1030,  1045,  2196,  2354,\n","         2151,  2028,  2450,  1005,  1055,  2293,  1030,  6561,  2091,  1999,\n","         2026,  2540,  1006,  2126,  2091,  1007,  1010,  8769,  2039,  2026,\n","         3969,  1010,  1030,  1030,  1030,  2507,  2009,  2039,  3336,  1010,\n","         3336,  1010,  2507,  2009,  2039,  1012,  1030,  1006,  1045,  2056,\n","         3336,  2507,  2009,  2039,  1007,  1030,  2507,  2009,  2039,  3336,\n","         1010,  3336,  1010,  2507,  2009,  2039,  1012,  1030,  1006,  1045,\n","         2056,  3336,  2507,  2009,  2039,  1007,  1030,  1045,  1005,  1049,\n","         2746,  2067,  2005,  2062,  1010,  2061,  2330,  2039,  2115,  2341,\n","         1010,  1051, 11631,  3336,  1012,  1030,  1045,  2288,  2019,  2412,\n","         3201,  2293,  1010, 25391,  1010,  2040,  2080,  1011,  1051, 11631,\n","         3336,  1012,  1030,  1045,  2288,  2019,  2412,  3201,  2293,  1012,\n","         1030,  7893,  1016,  1024,  1030,  2129,  2106,  1045,  2412,  2104,\n","         4355, 21499,  1030,  2043,  1010,  9548,  1010,  9044,  2013,  9044,\n","         1010,  1045,  6449,  1045,  1005,  2310,  2179,  1030,  1030,  2507,\n","         2009,  2039,  3336,  1010,  3336,  1010,  2507,  2009,  2039,  1012,\n","         1030,  1006,  1045,  2056,  3336,  2507,  2009,  2039,  1007,  1030,\n","         2507,  2009,  2039,  3336,  1010,  3336,  1010,  2507,  2009,  2039,\n","         1012,  1030,  1006,  1045,  2056,  3336,  2507,  2009,  2039,  1007,\n","         1030,  1045,  1005,  1049,  2746,  2067,  2005,  2062,  1010,  2061,\n","         2330,  2039,  2115,  2341,  1010,  1051, 11631,  3336,  1012,  1030,\n","         1045,  2288,  2019,  2412,  3201,  2293,  1010,  2202,  2009,  3336,\n","         1010,  2040,  2080,  1011,  1051, 11631,  3336,  1012,  1030,  1045,\n","         2288,  2019,  2412,  3201,  2293,  1012,  1030,  1030,  2412,  3201,\n","         2293,  1012,  1030,  2412,  3201,  2293,  1012,  1030,  1030,  2507,\n","         2009,  2039,  3336,  1010,  3336,  1010,  2507,  2009,  2039,  1012,\n","         1030,  1006,  1045,  2056,  3336,  2507,  2009,  2039,  1007,  1030,\n","         2507,  2009,  2039,  3336,  1010,  3336,  1010,  2507,  2009,  2039,\n","         1012,  1030,  1006,  1045,  2056,  3336,  2507,  2009,  2039,  1007,\n","         1030,  1045,  2288,  2293,  2006,  2026,  2568,  1010,  1045,  2342,\n","         2017,  2011,  2026,  2217,  1012,  1030,  1045,  2288,  2019,  2412,\n","         3201,  2293,  1010,  2202,  2009,  3336,  1010,  2040,  2080,  1011,\n","         1051, 11631,  3336,  1012,  1030,  1045,  2288,  2019,  2412,  3201,\n","         2293,  1012,  1030,  2041,  3217,  1024,  1030,  1045,  2288,  2019,\n","         2412,  3201,  2293,  1012,  1030,  1045,  2288,  2019,  2412,  3201,\n","         2293,  1012,  1030,  1045,  2288,  2019,  2412,  3201,  2293,  1012,\n","         1006, 26784,  2041,  1007,  1030,  1030,  1030,  1030,  1030,  1030,\n","         1030,  1030,  1030,  1030,  1030,  1030,  1030,  1030,  1030,  1030,\n","         1030,  1030,  1030,   102,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,   79, 7986,\n","           0,    0, 8051,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,   79, 7986,    0,    0, 8051,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0, 2200,   14,    0,   79,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,   79,    0,    0,\n","           0,    0,   14,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,   79,    0,    0,    0,    0,   14,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,   79, 7986,    0,    0, 8051,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          79, 7986,    0,    0, 8051,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0, 2200,   14,    0,   79,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,   79,    0,    0,    0,    0,   14,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,   79,    0,    0,\n","           0,    0,   14,    0,    0,   79,    0,    0,    0,    0,   79,    0,\n","           0,    0,    0,    0,    0,   79, 7986,    0,    0, 8051,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,   79, 7986,    0,    0, 8051,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 2200,   14,\n","           0,   79,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          79,    0,    0,    0,    0,   14,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,   79,    0,    0,    0,    0,\n","          14,    0,    0,    0,    0,    0,    0,   79,    0,    0,    0,    0,\n","           0,    0,    0,   79,    0,    0,    0,    0,    0,    0,    0,   14,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0])}\n","10178\n"]}]},{"cell_type":"code","source":["\n","NOTES_NUM = 12\n","CHORD_TYPES_NUM = 142\n","OUTPUT_DIM = 1+CHORD_TYPES_NUM*NOTES_NUM**2\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","\n","model = DistilBertForTokenClassification.from_pretrained(\n","    'bert-base-uncased',\n","    num_labels = OUTPUT_DIM,\n","    output_attentions = False,\n","    output_hidden_states = False,\n","    )\n","model.to(device)\n","optimizer = torch.optim.AdamW(model.parameters(),\n","                              lr = 1e-5,\n","                              eps = 1e-08\n","                              )\n","BATCH_SIZE = 8\n","N_EPOCHS = 5\n","alpha = 0.75\n"],"metadata":{"id":"YZ8718aLDCCo","colab":{"base_uri":"https://localhost:8080/","height":124,"referenced_widgets":["1987063f9aa74301aa986afeda773094","6d0762a3a9a24be9b3b685b314234c4d","eba34e134a634ed4adbd843aa29b77d0","7472bfeb06b24fbe94c02a46e295d421","8785699d2a124594a3c21b6332871e6b","03a28944279b4f3494c0dad3da53ee0f","8000fcd65a5d4780a7c1c7b72827c877","ebffb63d5696411cbae0cfc65461f9ca","abd8308d088345fcab985b8353306f02","da021e63a8724d9ca736186596ca162f","2a225b8640b74170be280bd1642d2119"]},"executionInfo":{"status":"ok","timestamp":1697892008773,"user_tz":-180,"elapsed":10952,"user":{"displayName":"Amit Tal","userId":"10134168706322622051"}},"outputId":"c8da48b7-3ede-4e87-af76-bb9fb2db45b5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["You are using a model of type bert to instantiate a model of type distilbert. This is not supported for all configurations of models and can yield errors.\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1987063f9aa74301aa986afeda773094"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['transformer.layer.9.attention.v_lin.weight', 'transformer.layer.11.output_layer_norm.weight', 'transformer.layer.7.output_layer_norm.bias', 'transformer.layer.4.ffn.lin2.bias', 'transformer.layer.5.attention.k_lin.weight', 'transformer.layer.4.output_layer_norm.weight', 'embeddings.word_embeddings.weight', 'transformer.layer.3.output_layer_norm.bias', 'transformer.layer.11.attention.q_lin.weight', 'transformer.layer.5.ffn.lin2.weight', 'transformer.layer.6.ffn.lin1.weight', 'transformer.layer.8.attention.q_lin.weight', 'transformer.layer.9.attention.out_lin.weight', 'transformer.layer.11.sa_layer_norm.weight', 'transformer.layer.7.output_layer_norm.weight', 'transformer.layer.4.sa_layer_norm.bias', 'transformer.layer.2.attention.v_lin.weight', 'transformer.layer.10.attention.out_lin.bias', 'transformer.layer.1.output_layer_norm.bias', 'transformer.layer.3.attention.q_lin.bias', 'transformer.layer.6.attention.v_lin.bias', 'transformer.layer.11.attention.q_lin.bias', 'transformer.layer.1.sa_layer_norm.bias', 'transformer.layer.3.ffn.lin2.bias', 'transformer.layer.5.attention.out_lin.bias', 'transformer.layer.6.output_layer_norm.bias', 'transformer.layer.7.ffn.lin1.weight', 'transformer.layer.9.sa_layer_norm.weight', 'transformer.layer.2.sa_layer_norm.bias', 'transformer.layer.4.attention.v_lin.weight', 'transformer.layer.11.ffn.lin2.bias', 'transformer.layer.2.attention.out_lin.bias', 'transformer.layer.3.attention.v_lin.weight', 'transformer.layer.5.attention.q_lin.bias', 'transformer.layer.10.attention.q_lin.bias', 'transformer.layer.3.sa_layer_norm.bias', 'transformer.layer.8.attention.k_lin.bias', 'transformer.layer.4.ffn.lin1.bias', 'transformer.layer.10.attention.v_lin.weight', 'transformer.layer.1.attention.out_lin.bias', 'transformer.layer.1.attention.v_lin.bias', 'transformer.layer.2.attention.q_lin.bias', 'transformer.layer.6.sa_layer_norm.weight', 'transformer.layer.0.attention.k_lin.bias', 'transformer.layer.5.ffn.lin1.bias', 'transformer.layer.11.ffn.lin1.bias', 'transformer.layer.1.attention.q_lin.bias', 'transformer.layer.11.attention.out_lin.bias', 'transformer.layer.1.ffn.lin1.bias', 'transformer.layer.3.attention.k_lin.bias', 'transformer.layer.9.output_layer_norm.weight', 'transformer.layer.9.ffn.lin1.weight', 'transformer.layer.1.ffn.lin2.weight', 'transformer.layer.6.sa_layer_norm.bias', 'transformer.layer.8.attention.v_lin.weight', 'transformer.layer.5.attention.k_lin.bias', 'transformer.layer.0.attention.v_lin.weight', 'transformer.layer.3.attention.k_lin.weight', 'transformer.layer.4.attention.k_lin.weight', 'classifier.weight', 'transformer.layer.0.ffn.lin2.weight', 'transformer.layer.6.ffn.lin2.bias', 'transformer.layer.4.ffn.lin1.weight', 'transformer.layer.11.attention.v_lin.bias', 'transformer.layer.5.attention.out_lin.weight', 'transformer.layer.1.sa_layer_norm.weight', 'transformer.layer.8.attention.v_lin.bias', 'transformer.layer.9.attention.q_lin.bias', 'transformer.layer.7.sa_layer_norm.weight', 'transformer.layer.2.output_layer_norm.weight', 'transformer.layer.4.attention.q_lin.weight', 'transformer.layer.1.attention.v_lin.weight', 'transformer.layer.0.output_layer_norm.weight', 'transformer.layer.9.attention.q_lin.weight', 'transformer.layer.6.output_layer_norm.weight', 'transformer.layer.8.ffn.lin1.bias', 'transformer.layer.8.ffn.lin1.weight', 'transformer.layer.11.attention.v_lin.weight', 'transformer.layer.0.output_layer_norm.bias', 'transformer.layer.5.ffn.lin2.bias', 'transformer.layer.10.attention.k_lin.weight', 'transformer.layer.11.output_layer_norm.bias', 'transformer.layer.0.attention.v_lin.bias', 'classifier.bias', 'transformer.layer.5.output_layer_norm.weight', 'transformer.layer.7.ffn.lin2.bias', 'transformer.layer.8.attention.out_lin.bias', 'transformer.layer.9.ffn.lin2.bias', 'transformer.layer.6.attention.out_lin.bias', 'transformer.layer.4.ffn.lin2.weight', 'transformer.layer.2.attention.k_lin.weight', 'transformer.layer.9.ffn.lin2.weight', 'transformer.layer.2.attention.v_lin.bias', 'transformer.layer.7.attention.v_lin.bias', 'transformer.layer.3.ffn.lin2.weight', 'transformer.layer.9.attention.v_lin.bias', 'transformer.layer.9.sa_layer_norm.bias', 'transformer.layer.4.sa_layer_norm.weight', 'transformer.layer.10.ffn.lin1.bias', 'transformer.layer.8.output_layer_norm.weight', 'transformer.layer.6.attention.v_lin.weight', 'transformer.layer.8.output_layer_norm.bias', 'transformer.layer.2.sa_layer_norm.weight', 'transformer.layer.3.output_layer_norm.weight', 'transformer.layer.5.ffn.lin1.weight', 'transformer.layer.2.ffn.lin2.bias', 'transformer.layer.10.attention.k_lin.bias', 'transformer.layer.3.attention.out_lin.weight', 'transformer.layer.7.attention.k_lin.bias', 'transformer.layer.9.attention.k_lin.weight', 'transformer.layer.10.attention.out_lin.weight', 'transformer.layer.5.attention.q_lin.weight', 'transformer.layer.0.ffn.lin2.bias', 'transformer.layer.1.attention.k_lin.bias', 'transformer.layer.3.attention.v_lin.bias', 'transformer.layer.5.sa_layer_norm.weight', 'transformer.layer.3.ffn.lin1.weight', 'transformer.layer.3.attention.q_lin.weight', 'transformer.layer.6.ffn.lin1.bias', 'transformer.layer.0.attention.out_lin.weight', 'transformer.layer.6.attention.k_lin.weight', 'transformer.layer.6.attention.k_lin.bias', 'transformer.layer.9.attention.out_lin.bias', 'transformer.layer.1.attention.out_lin.weight', 'transformer.layer.7.sa_layer_norm.bias', 'transformer.layer.10.sa_layer_norm.bias', 'transformer.layer.2.attention.k_lin.bias', 'transformer.layer.9.ffn.lin1.bias', 'transformer.layer.1.ffn.lin2.bias', 'embeddings.LayerNorm.weight', 'transformer.layer.5.sa_layer_norm.bias', 'transformer.layer.4.attention.k_lin.bias', 'transformer.layer.10.ffn.lin1.weight', 'transformer.layer.7.attention.out_lin.bias', 'transformer.layer.7.ffn.lin2.weight', 'transformer.layer.3.ffn.lin1.bias', 'transformer.layer.7.ffn.lin1.bias', 'transformer.layer.0.sa_layer_norm.weight', 'transformer.layer.4.attention.q_lin.bias', 'transformer.layer.0.attention.out_lin.bias', 'transformer.layer.2.output_layer_norm.bias', 'transformer.layer.7.attention.q_lin.weight', 'transformer.layer.7.attention.v_lin.weight', 'transformer.layer.5.attention.v_lin.weight', 'transformer.layer.8.attention.q_lin.bias', 'transformer.layer.10.sa_layer_norm.weight', 'transformer.layer.7.attention.k_lin.weight', 'transformer.layer.10.ffn.lin2.bias', 'transformer.layer.8.sa_layer_norm.bias', 'transformer.layer.6.attention.q_lin.bias', 'transformer.layer.8.attention.out_lin.weight', 'transformer.layer.6.attention.out_lin.weight', 'transformer.layer.4.attention.out_lin.bias', 'transformer.layer.11.ffn.lin2.weight', 'transformer.layer.11.attention.out_lin.weight', 'transformer.layer.4.attention.out_lin.weight', 'embeddings.LayerNorm.bias', 'transformer.layer.4.attention.v_lin.bias', 'transformer.layer.9.output_layer_norm.bias', 'transformer.layer.2.ffn.lin1.weight', 'transformer.layer.11.attention.k_lin.bias', 'transformer.layer.2.attention.q_lin.weight', 'transformer.layer.7.attention.q_lin.bias', 'transformer.layer.8.ffn.lin2.weight', 'transformer.layer.10.attention.v_lin.bias', 'transformer.layer.6.ffn.lin2.weight', 'transformer.layer.3.attention.out_lin.bias', 'transformer.layer.8.attention.k_lin.weight', 'transformer.layer.8.ffn.lin2.bias', 'transformer.layer.7.attention.out_lin.weight', 'transformer.layer.10.attention.q_lin.weight', 'transformer.layer.10.ffn.lin2.weight', 'transformer.layer.1.attention.q_lin.weight', 'transformer.layer.1.ffn.lin1.weight', 'transformer.layer.6.attention.q_lin.weight', 'transformer.layer.0.attention.q_lin.bias', 'transformer.layer.0.ffn.lin1.weight', 'transformer.layer.0.sa_layer_norm.bias', 'transformer.layer.2.ffn.lin2.weight', 'transformer.layer.1.output_layer_norm.weight', 'transformer.layer.0.attention.q_lin.weight', 'transformer.layer.11.sa_layer_norm.bias', 'transformer.layer.11.ffn.lin1.weight', 'transformer.layer.0.attention.k_lin.weight', 'transformer.layer.9.attention.k_lin.bias', 'transformer.layer.4.output_layer_norm.bias', 'transformer.layer.5.output_layer_norm.bias', 'transformer.layer.1.attention.k_lin.weight', 'transformer.layer.10.output_layer_norm.bias', 'embeddings.position_embeddings.weight', 'transformer.layer.11.attention.k_lin.weight', 'transformer.layer.3.sa_layer_norm.weight', 'transformer.layer.10.output_layer_norm.weight', 'transformer.layer.8.sa_layer_norm.weight', 'transformer.layer.2.ffn.lin1.bias', 'transformer.layer.0.ffn.lin1.bias', 'transformer.layer.5.attention.v_lin.bias', 'transformer.layer.2.attention.out_lin.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["\n","train_dataloader = DataLoader(dataset['train'], batch_size=BATCH_SIZE)\n","eval_dataloader = DataLoader(dataset['val'], batch_size=BATCH_SIZE)\n","best_val_loss = float(\"inf\")\n","non_zero_loss = nn.CrossEntropyLoss()\n","\n","for epoch in range(N_EPOCHS):\n","    print(f\"starting epoch {epoch}\")\n","\n","    # ========== Training ==========\n","\n","    # Set model to training mode\n","    model.train()\n","\n","    # Tracking variables\n","    tr_loss = 0\n","    nb_tr_examples, nb_tr_steps = 0, 0\n","\n","\n","\n","    for step, batch in enumerate(train_dataloader):\n","        b_input_ids = batch['input_ids'].to(device)\n","        b_input_mask = batch['attention_mask'].to(device)\n","        b_labels = batch['labels'].to(device)\n","        optimizer.zero_grad()\n","        # Forward pass\n","        train_output = model(b_input_ids,\n","                             attention_mask = b_input_mask,\n","                             labels = b_labels)\n","        # Backward pass\n","        input = train_output.logits.permute([0,2,1])\n","        b_zero_labels = torch.zeros(input.shape[0], max_input_length).long().to(device).detach()\n","        b_loss = train_output.loss - alpha*non_zero_loss(input, b_zero_labels)\n","        b_loss.backward()\n","        optimizer.step()\n","        # Update tracking variables\n","        #tr_loss += train_output.loss.item()\n","        tr_loss += b_loss.item()\n","        nb_tr_examples += b_input_ids.size(0)\n","        nb_tr_steps += 1\n","\n","    # ========== Validation ==========\n","\n","    # Set model to evaluation mode\n","    model.eval()\n","\n","    # Tracking variables\n","    val_accuracy = []\n","    val_precision = []\n","    val_recall = []\n","    val_specificity = []\n","\n","    # Tracking variables\n","    eval_loss = 0\n","    nb_eval_examples, nb_eval_steps = 0, 0\n","    for batch_i, batch in enumerate(eval_dataloader):\n","        b_input_ids = batch['input_ids'].to(device)\n","        b_input_mask = batch['attention_mask'].to(device)\n","        b_labels = batch['labels'].to(device)\n","        with torch.no_grad():\n","          # Forward pass\n","          eval_output = model(b_input_ids,\n","                              attention_mask = b_input_mask,\n","                              labels = b_labels)\n","        input = eval_output.logits.permute([0,2,1])\n","        b_zero_labels = torch.zeros(input.shape[0], max_input_length).long().to(device).detach()\n","        b_loss = eval_output.loss - alpha*non_zero_loss(input, b_zero_labels)\n","        # Update tracking variables)\n","        #eval_loss += eval_output.loss.item()\n","        eval_loss += b_loss.item()\n","        nb_eval_examples += b_input_ids.size(0)\n","        nb_eval_steps += 1\n","    avg_eval_loss =  eval_loss / nb_eval_steps\n","    if avg_eval_loss < best_val_loss:\n","      best_val_loss = avg_eval_loss\n","      print(\"Saving checkpoint!\")\n","      torch.save({\n","          'epoch': epoch,\n","          'model_state_dict': model.state_dict(),\n","          'optimizer_state_dict': optimizer.state_dict(),\n","          'val_accuracy': val_accuracy,\n","          },\n","          f\"checkpoints/2_epoch_{epoch}.pt\"\n","      )\n","\n","    print('\\n\\t - Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n","    print('\\t - Validation loss: {:.4f}'.format(eval_loss / nb_eval_steps))"],"metadata":{"id":"PnrUOCL8WoKT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697842617182,"user_tz":-180,"elapsed":8903326,"user":{"displayName":"Amit Tal","userId":"10134168706322622051"}},"outputId":"a814d1dd-0d74-4181-8794-aae4513d7bd4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["starting epoch 0\n","Saving checkpoint!\n","\n","\t - Train loss: 1.3692\n","\t - Validation loss: 1.2392\n","starting epoch 1\n","Saving checkpoint!\n","\n","\t - Train loss: 1.2347\n","\t - Validation loss: 1.2292\n","starting epoch 2\n","Saving checkpoint!\n","\n","\t - Train loss: 1.2207\n","\t - Validation loss: 1.2240\n","starting epoch 3\n","\n","\t - Train loss: 1.2078\n","\t - Validation loss: 1.2258\n","starting epoch 4\n","\n","\t - Train loss: 1.1949\n","\t - Validation loss: 1.2303\n"]}]},{"cell_type":"code","source":["model = DistilBertForTokenClassification.from_pretrained(\n","    'bert-base-uncased',\n","    num_labels = OUTPUT_DIM,\n","    output_attentions = False,\n","    output_hidden_states = False,\n","    )\n","optimizer = torch.optim.AdamW(model.parameters(),\n","                              lr = 1e-5,\n","                              eps = 1e-08\n","                              )\n","\n","for i in range(N_EPOCHS):\n","  try:\n","    checkpoint = torch.load(f\"checkpoints/epoch_{i}.pt\", map_location='cpu')\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    print(f\"load model for checkpoints {i}\")\n","  except:\n","    print(f\"no model for checkpoints {i}\")\n","model.to(device)"],"metadata":{"id":"WtT135QUqnf5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697892202142,"user_tz":-180,"elapsed":24760,"user":{"displayName":"Amit Tal","userId":"10134168706322622051"}},"outputId":"38c3b3f3-c313-490b-9562-b129dc2627f2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["You are using a model of type bert to instantiate a model of type distilbert. This is not supported for all configurations of models and can yield errors.\n","Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['transformer.layer.9.attention.v_lin.weight', 'transformer.layer.11.output_layer_norm.weight', 'transformer.layer.7.output_layer_norm.bias', 'transformer.layer.4.ffn.lin2.bias', 'transformer.layer.5.attention.k_lin.weight', 'transformer.layer.4.output_layer_norm.weight', 'embeddings.word_embeddings.weight', 'transformer.layer.3.output_layer_norm.bias', 'transformer.layer.11.attention.q_lin.weight', 'transformer.layer.5.ffn.lin2.weight', 'transformer.layer.6.ffn.lin1.weight', 'transformer.layer.8.attention.q_lin.weight', 'transformer.layer.9.attention.out_lin.weight', 'transformer.layer.11.sa_layer_norm.weight', 'transformer.layer.7.output_layer_norm.weight', 'transformer.layer.4.sa_layer_norm.bias', 'transformer.layer.2.attention.v_lin.weight', 'transformer.layer.10.attention.out_lin.bias', 'transformer.layer.1.output_layer_norm.bias', 'transformer.layer.3.attention.q_lin.bias', 'transformer.layer.6.attention.v_lin.bias', 'transformer.layer.11.attention.q_lin.bias', 'transformer.layer.1.sa_layer_norm.bias', 'transformer.layer.3.ffn.lin2.bias', 'transformer.layer.5.attention.out_lin.bias', 'transformer.layer.6.output_layer_norm.bias', 'transformer.layer.7.ffn.lin1.weight', 'transformer.layer.9.sa_layer_norm.weight', 'transformer.layer.2.sa_layer_norm.bias', 'transformer.layer.4.attention.v_lin.weight', 'transformer.layer.11.ffn.lin2.bias', 'transformer.layer.2.attention.out_lin.bias', 'transformer.layer.3.attention.v_lin.weight', 'transformer.layer.5.attention.q_lin.bias', 'transformer.layer.10.attention.q_lin.bias', 'transformer.layer.3.sa_layer_norm.bias', 'transformer.layer.8.attention.k_lin.bias', 'transformer.layer.4.ffn.lin1.bias', 'transformer.layer.10.attention.v_lin.weight', 'transformer.layer.1.attention.out_lin.bias', 'transformer.layer.1.attention.v_lin.bias', 'transformer.layer.2.attention.q_lin.bias', 'transformer.layer.6.sa_layer_norm.weight', 'transformer.layer.0.attention.k_lin.bias', 'transformer.layer.5.ffn.lin1.bias', 'transformer.layer.11.ffn.lin1.bias', 'transformer.layer.1.attention.q_lin.bias', 'transformer.layer.11.attention.out_lin.bias', 'transformer.layer.1.ffn.lin1.bias', 'transformer.layer.3.attention.k_lin.bias', 'transformer.layer.9.output_layer_norm.weight', 'transformer.layer.9.ffn.lin1.weight', 'transformer.layer.1.ffn.lin2.weight', 'transformer.layer.6.sa_layer_norm.bias', 'transformer.layer.8.attention.v_lin.weight', 'transformer.layer.5.attention.k_lin.bias', 'transformer.layer.0.attention.v_lin.weight', 'transformer.layer.3.attention.k_lin.weight', 'transformer.layer.4.attention.k_lin.weight', 'classifier.weight', 'transformer.layer.0.ffn.lin2.weight', 'transformer.layer.6.ffn.lin2.bias', 'transformer.layer.4.ffn.lin1.weight', 'transformer.layer.11.attention.v_lin.bias', 'transformer.layer.5.attention.out_lin.weight', 'transformer.layer.1.sa_layer_norm.weight', 'transformer.layer.8.attention.v_lin.bias', 'transformer.layer.9.attention.q_lin.bias', 'transformer.layer.7.sa_layer_norm.weight', 'transformer.layer.2.output_layer_norm.weight', 'transformer.layer.4.attention.q_lin.weight', 'transformer.layer.1.attention.v_lin.weight', 'transformer.layer.0.output_layer_norm.weight', 'transformer.layer.9.attention.q_lin.weight', 'transformer.layer.6.output_layer_norm.weight', 'transformer.layer.8.ffn.lin1.bias', 'transformer.layer.8.ffn.lin1.weight', 'transformer.layer.11.attention.v_lin.weight', 'transformer.layer.0.output_layer_norm.bias', 'transformer.layer.5.ffn.lin2.bias', 'transformer.layer.10.attention.k_lin.weight', 'transformer.layer.11.output_layer_norm.bias', 'transformer.layer.0.attention.v_lin.bias', 'classifier.bias', 'transformer.layer.5.output_layer_norm.weight', 'transformer.layer.7.ffn.lin2.bias', 'transformer.layer.8.attention.out_lin.bias', 'transformer.layer.9.ffn.lin2.bias', 'transformer.layer.6.attention.out_lin.bias', 'transformer.layer.4.ffn.lin2.weight', 'transformer.layer.2.attention.k_lin.weight', 'transformer.layer.9.ffn.lin2.weight', 'transformer.layer.2.attention.v_lin.bias', 'transformer.layer.7.attention.v_lin.bias', 'transformer.layer.3.ffn.lin2.weight', 'transformer.layer.9.attention.v_lin.bias', 'transformer.layer.9.sa_layer_norm.bias', 'transformer.layer.4.sa_layer_norm.weight', 'transformer.layer.10.ffn.lin1.bias', 'transformer.layer.8.output_layer_norm.weight', 'transformer.layer.6.attention.v_lin.weight', 'transformer.layer.8.output_layer_norm.bias', 'transformer.layer.2.sa_layer_norm.weight', 'transformer.layer.3.output_layer_norm.weight', 'transformer.layer.5.ffn.lin1.weight', 'transformer.layer.2.ffn.lin2.bias', 'transformer.layer.10.attention.k_lin.bias', 'transformer.layer.3.attention.out_lin.weight', 'transformer.layer.7.attention.k_lin.bias', 'transformer.layer.9.attention.k_lin.weight', 'transformer.layer.10.attention.out_lin.weight', 'transformer.layer.5.attention.q_lin.weight', 'transformer.layer.0.ffn.lin2.bias', 'transformer.layer.1.attention.k_lin.bias', 'transformer.layer.3.attention.v_lin.bias', 'transformer.layer.5.sa_layer_norm.weight', 'transformer.layer.3.ffn.lin1.weight', 'transformer.layer.3.attention.q_lin.weight', 'transformer.layer.6.ffn.lin1.bias', 'transformer.layer.0.attention.out_lin.weight', 'transformer.layer.6.attention.k_lin.weight', 'transformer.layer.6.attention.k_lin.bias', 'transformer.layer.9.attention.out_lin.bias', 'transformer.layer.1.attention.out_lin.weight', 'transformer.layer.7.sa_layer_norm.bias', 'transformer.layer.10.sa_layer_norm.bias', 'transformer.layer.2.attention.k_lin.bias', 'transformer.layer.9.ffn.lin1.bias', 'transformer.layer.1.ffn.lin2.bias', 'embeddings.LayerNorm.weight', 'transformer.layer.5.sa_layer_norm.bias', 'transformer.layer.4.attention.k_lin.bias', 'transformer.layer.10.ffn.lin1.weight', 'transformer.layer.7.attention.out_lin.bias', 'transformer.layer.7.ffn.lin2.weight', 'transformer.layer.3.ffn.lin1.bias', 'transformer.layer.7.ffn.lin1.bias', 'transformer.layer.0.sa_layer_norm.weight', 'transformer.layer.4.attention.q_lin.bias', 'transformer.layer.0.attention.out_lin.bias', 'transformer.layer.2.output_layer_norm.bias', 'transformer.layer.7.attention.q_lin.weight', 'transformer.layer.7.attention.v_lin.weight', 'transformer.layer.5.attention.v_lin.weight', 'transformer.layer.8.attention.q_lin.bias', 'transformer.layer.10.sa_layer_norm.weight', 'transformer.layer.7.attention.k_lin.weight', 'transformer.layer.10.ffn.lin2.bias', 'transformer.layer.8.sa_layer_norm.bias', 'transformer.layer.6.attention.q_lin.bias', 'transformer.layer.8.attention.out_lin.weight', 'transformer.layer.6.attention.out_lin.weight', 'transformer.layer.4.attention.out_lin.bias', 'transformer.layer.11.ffn.lin2.weight', 'transformer.layer.11.attention.out_lin.weight', 'transformer.layer.4.attention.out_lin.weight', 'embeddings.LayerNorm.bias', 'transformer.layer.4.attention.v_lin.bias', 'transformer.layer.9.output_layer_norm.bias', 'transformer.layer.2.ffn.lin1.weight', 'transformer.layer.11.attention.k_lin.bias', 'transformer.layer.2.attention.q_lin.weight', 'transformer.layer.7.attention.q_lin.bias', 'transformer.layer.8.ffn.lin2.weight', 'transformer.layer.10.attention.v_lin.bias', 'transformer.layer.6.ffn.lin2.weight', 'transformer.layer.3.attention.out_lin.bias', 'transformer.layer.8.attention.k_lin.weight', 'transformer.layer.8.ffn.lin2.bias', 'transformer.layer.7.attention.out_lin.weight', 'transformer.layer.10.attention.q_lin.weight', 'transformer.layer.10.ffn.lin2.weight', 'transformer.layer.1.attention.q_lin.weight', 'transformer.layer.1.ffn.lin1.weight', 'transformer.layer.6.attention.q_lin.weight', 'transformer.layer.0.attention.q_lin.bias', 'transformer.layer.0.ffn.lin1.weight', 'transformer.layer.0.sa_layer_norm.bias', 'transformer.layer.2.ffn.lin2.weight', 'transformer.layer.1.output_layer_norm.weight', 'transformer.layer.0.attention.q_lin.weight', 'transformer.layer.11.sa_layer_norm.bias', 'transformer.layer.11.ffn.lin1.weight', 'transformer.layer.0.attention.k_lin.weight', 'transformer.layer.9.attention.k_lin.bias', 'transformer.layer.4.output_layer_norm.bias', 'transformer.layer.5.output_layer_norm.bias', 'transformer.layer.1.attention.k_lin.weight', 'transformer.layer.10.output_layer_norm.bias', 'embeddings.position_embeddings.weight', 'transformer.layer.11.attention.k_lin.weight', 'transformer.layer.3.sa_layer_norm.weight', 'transformer.layer.10.output_layer_norm.weight', 'transformer.layer.8.sa_layer_norm.weight', 'transformer.layer.2.ffn.lin1.bias', 'transformer.layer.0.ffn.lin1.bias', 'transformer.layer.5.attention.v_lin.bias', 'transformer.layer.2.attention.out_lin.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["load model for checkpoints 0\n","load model for checkpoints 1\n","load model for checkpoints 2\n","no model for checkpoints 3\n","no model for checkpoints 4\n"]},{"output_type":"execute_result","data":{"text/plain":["DistilBertForTokenClassification(\n","  (distilbert): DistilBertModel(\n","    (embeddings): Embeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (transformer): Transformer(\n","      (layer): ModuleList(\n","        (0-11): 12 x TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","            (activation): GELUActivation()\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","      )\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=20449, bias=True)\n",")"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["#https://www.song-lyrics-generator.org.uk/\n","song = '''We're all going to a summer road trip\n"," @ No more shouting for a week or two\n"," @ Super bananas and ugly ants at our summer road trip\n"," @ No more meaty ants for me or you\n"," @ For a week or two\n","\n"," @ Summertime, and the livin' is super\n"," @ Bananas are bouncing and the ants are high\n"," @ Oh, your brother-in-laws smelly and your instructor is bright\n"," @ So hush my crazy dearest, don't you cry\n","\n"," @ Oh the summer of 2000\n","\n"," @ I cant wait to do some bouncing with you\n"," @ You cant wait to do some bouncing with me\n"," @ This just cant be summer love, youll see\n"," @ This just cant be summer love\n","\n"," @ Cause you were mine for the summer\n"," @ Now we know its nearly over\n"," @ Feels like Sunday mist\n"," @ But I always will remember\n"," @ You were my summer love\n"," @ You always will be my summer love\n","\n"," @ I wish they all could be\n"," @ I wish they all could be\n"," @ I wish they all could be bananas of Canada\n","\n"," @ Summertime, and the livin is super\n"," @ Bananas are bouncing and the ants are high\n"," @ Oh, your brother-in-law's smelly and your instructor is bright\n"," @ So hush crazy dearest, don't you cry\n","\n"," @ Me and some ants from Skegness\n"," @ Had a band and we tried real hard.\n"," @ Arthur quit, Lisa went daring\n"," @ I should've known we'd never end up collapsing\n","\n"," @ Oh the summer of 2000\n","\n"," @ Summer bouncing had me a blast, oh yeah\n"," @ Summer bouncing happened so fast,\n","\n"," @ Summer road trip drifting away,\n"," @ To, uh oh, that summer road trip\n","\n"," @ Yeah the summer of 2000'''"],"metadata":{"id":"Vf-V40AJx9xO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenized_song = tokenizer(song,\n","                           padding='max_length',\n","                           truncation=True)\n","\n","input_ids = torch.IntTensor(tokenized_song['input_ids'])\n","input_ids = torch.reshape(input_ids, (1, -1))\n","\n","token_type_ids = torch.IntTensor(tokenized_song['token_type_ids'])\n","token_type_ids = torch.reshape(token_type_ids, (1, -1))\n","\n","attention_mask = torch.IntTensor(tokenized_song['attention_mask'])\n","attention_mask = torch.reshape(attention_mask, (1, -1))\n","with torch.no_grad():\n","  song_chords = model(input_ids.to(device), attention_mask = attention_mask.to(device)).logits\n","song_chords = torch.reshape(song_chords, (max_input_length, -1))\n","song_chords = song_chords.argmax(1)\n","song_chords = np.array(song_chords.tolist())\n","lyrics = np.array(tokenizer.tokenize(song))\n","song_chords = song_chords[0:len(lyrics)]\n","print(lyrics)\n","print(song_chords)\n","####################################\n","#last_chord = 0\n","#for i in range(len(song_chords)):\n","#  if song_chords[i] == last_chord:\n","#    song_chords[i] = 0\n","#  else:\n","#    last_chord = song_chords[i]\n","####################################\n","lyrich_with_song = np.array([lyrics[i] + \"/\" +  ChordTokenizer.detokenize(song_chords[i]) for i in range(len(song_chords))])\n","\n","result = \"\"\n","for word in lyrich_with_song:\n","  if word[0] == '@':\n","    result +='\\n'\n","  else:\n","    word = \" \" + word\n","    if word[-1] == \"/\":\n","      word = word[0:-1]\n","    result +=  word\n","print(result)\n"],"metadata":{"id":"OISN8obkzeCP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697892202142,"user_tz":-180,"elapsed":11,"user":{"displayName":"Amit Tal","userId":"10134168706322622051"}},"outputId":"cedac853-6285-47a3-fc64-89af334264b3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['we' \"'\" 're' 'all' 'going' 'to' 'a' 'summer' 'road' 'trip' '@' 'no'\n"," 'more' 'shouting' 'for' 'a' 'week' 'or' 'two' '@' 'super' 'bananas' 'and'\n"," 'ugly' 'ants' 'at' 'our' 'summer' 'road' 'trip' '@' 'no' 'more' 'meat'\n"," '##y' 'ants' 'for' 'me' 'or' 'you' '@' 'for' 'a' 'week' 'or' 'two' '@'\n"," 'summer' '##time' ',' 'and' 'the' 'liv' '##in' \"'\" 'is' 'super' '@'\n"," 'bananas' 'are' 'bouncing' 'and' 'the' 'ants' 'are' 'high' '@' 'oh' ','\n"," 'your' 'brother' '-' 'in' '-' 'laws' 'smell' '##y' 'and' 'your'\n"," 'instructor' 'is' 'bright' '@' 'so' 'hush' 'my' 'crazy' 'dear' '##est'\n"," ',' 'don' \"'\" 't' 'you' 'cry' '@' 'oh' 'the' 'summer' 'of' '2000' '@' 'i'\n"," 'can' '##t' 'wait' 'to' 'do' 'some' 'bouncing' 'with' 'you' '@' 'you'\n"," 'can' '##t' 'wait' 'to' 'do' 'some' 'bouncing' 'with' 'me' '@' 'this'\n"," 'just' 'can' '##t' 'be' 'summer' 'love' ',' 'you' '##ll' 'see' '@' 'this'\n"," 'just' 'can' '##t' 'be' 'summer' 'love' '@' 'cause' 'you' 'were' 'mine'\n"," 'for' 'the' 'summer' '@' 'now' 'we' 'know' 'its' 'nearly' 'over' '@'\n"," 'feels' 'like' 'sunday' 'mist' '@' 'but' 'i' 'always' 'will' 'remember'\n"," '@' 'you' 'were' 'my' 'summer' 'love' '@' 'you' 'always' 'will' 'be' 'my'\n"," 'summer' 'love' '@' 'i' 'wish' 'they' 'all' 'could' 'be' '@' 'i' 'wish'\n"," 'they' 'all' 'could' 'be' '@' 'i' 'wish' 'they' 'all' 'could' 'be'\n"," 'bananas' 'of' 'canada' '@' 'summer' '##time' ',' 'and' 'the' 'liv'\n"," '##in' 'is' 'super' '@' 'bananas' 'are' 'bouncing' 'and' 'the' 'ants'\n"," 'are' 'high' '@' 'oh' ',' 'your' 'brother' '-' 'in' '-' 'law' \"'\" 's'\n"," 'smell' '##y' 'and' 'your' 'instructor' 'is' 'bright' '@' 'so' 'hush'\n"," 'crazy' 'dear' '##est' ',' 'don' \"'\" 't' 'you' 'cry' '@' 'me' 'and'\n"," 'some' 'ants' 'from' 'sk' '##eg' '##ness' '@' 'had' 'a' 'band' 'and' 'we'\n"," 'tried' 'real' 'hard' '.' '@' 'arthur' 'quit' ',' 'lisa' 'went' 'daring'\n"," '@' 'i' 'should' \"'\" 've' 'known' 'we' \"'\" 'd' 'never' 'end' 'up'\n"," 'collapsing' '@' 'oh' 'the' 'summer' 'of' '2000' '@' 'summer' 'bouncing'\n"," 'had' 'me' 'a' 'blast' ',' 'oh' 'yeah' '@' 'summer' 'bouncing' 'happened'\n"," 'so' 'fast' ',' '@' 'summer' 'road' 'trip' 'drifting' 'away' ',' '@' 'to'\n"," ',' 'uh' 'oh' ',' 'that' 'summer' 'road' 'trip' '@' 'yeah' 'the' 'summer'\n"," 'of' '2000']\n","[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0 131   0   0   0   0   0   0   0   0   0   0  40   0 131   0   0\n","   0   0   0   0   0 131   0   0   0   0 131 131   0   0   0 131 131   0\n","   0   0   0   0 131   0   0 131   0 131   0   0 131 131   0   0 131   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0 131   0   0   0   0   0 131   0   0   0   0   0\n","   0   0   0   0   0 131   0   0   0   0   0   0   0   0   0   0 131   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n"," 131   0   0   0   0   0   0   0 131   0   0   0   0   0   0 131   0 131\n","   0   0   0   0   0   0   0   0 131   0   0   0   0   0 131   0   0   0\n","   0   0   0   0   0   0   0   0 131   0 131   0   0   0   0 131   0   0\n","   0   0   0   0 131   0 131   0   0 131   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0 131   0   0 131   0   0   0\n","   0   0   0   0   0   0   0 131 131   0   0 131   0   0   0   0   0   0\n","   0   0   0   0   0 131   0 131   0   0   0   0   0   0 131   0   0   0\n"," 131 131   0   0 131   0 131   0   0   0 131   0   0 131 131   0   0   0\n","   0 131   0   0   0   0 131   0  40   0   0   0   0   0   0   0   0   0\n"," 131   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 131   0\n","   0   0   0   0   0   0   0   0   0   0 131 131   0   0   0   0   0]\n"," we ' re all going to a summer road trip\n"," no more shouting for a week or two\n"," super/G bananas and ugly ants at our summer road trip\n"," no/C more meat/G ##y ants for me or you\n"," for/G a week or two\n"," summer/G ##time , and the/G liv/G ##in ' is super\n"," bananas/G are bouncing and/G the ants/G are high\n"," oh/G , your brother/G - in - laws smell ##y and your instructor is bright\n"," so hush my crazy dear ##est , don ' t you cry\n"," oh/G the summer of 2000\n"," i/G can ##t wait to do some bouncing with you\n"," you/G can ##t wait to do some bouncing with me\n"," this/G just can ##t be summer love , you ##ll see\n"," this just can ##t be summer love\n"," cause/G you were mine for the summer\n"," now/G we know its nearly over\n"," feels/G like sunday/G mist\n"," but i always will remember\n"," you/G were my summer love\n"," you/G always will be my summer love\n"," i wish they all could/G be\n"," i wish they all could/G be\n"," i wish they all could/G be bananas/G of canada\n"," summer ##time , and the liv ##in is super\n"," bananas are bouncing and the ants are high\n"," oh/G , your brother/G - in - law ' s smell ##y and your instructor/G is/G bright\n"," so/G hush crazy dear ##est , don ' t you cry\n"," me/G and some/G ants from sk ##eg ##ness\n"," had/G a band and we/G tried/G real hard ./G\n"," arthur/G quit , lisa went/G daring\n"," i/G should/G ' ve known we '/G d never end up collapsing/G\n"," oh/C the summer of 2000\n"," summer bouncing had me a/G blast , oh yeah\n"," summer bouncing happened so fast ,\n"," summer road trip drifting/G away ,\n"," to , uh oh , that summer road trip/G\n"," yeah the summer of 2000\n"]}]}]}